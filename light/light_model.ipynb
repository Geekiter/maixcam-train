{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fd45b84-a71e-49ac-b360-26ef041227c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024/11/24 09:25:50 - INFO : TPU-MLIR v1.12-20241105\n",
      "2024/11/24 09:25:50 - INFO : \n",
      "\t _____________________________________________________ \n",
      "\t| preprocess:                                           |\n",
      "\t|   (x - mean) * scale                                  |\n",
      "\t'-------------------------------------------------------'\n",
      "  config Preprocess args : \n",
      "\tresize_dims           : same to net input dims\n",
      "\tkeep_aspect_ratio     : True\n",
      "\tkeep_ratio_mode       : letterbox\n",
      "\tpad_value             : 0\n",
      "\tpad_type              : center\n",
      "\t--------------------------\n",
      "\tmean                  : [0.0, 0.0, 0.0]\n",
      "\tscale                 : [0.003921569, 0.003921569, 0.003921569]\n",
      "\t--------------------------\n",
      "\tpixel_format          : rgb\n",
      "\tchannel_format        : nchw\n",
      "\n",
      "2024/11/24 09:25:51 - INFO : Input_shape assigned\n",
      "WARNING: onnx model check failed\n",
      "2024/11/24 09:25:51 - INFO : ConstantFolding finished\n",
      "2024/11/24 09:25:51 - INFO : skip_fuse_bn:False\n",
      "2024/11/24 09:25:51 - INFO : Onnxsim opt finished\n",
      "2024/11/24 09:25:52 - INFO : ConstantFolding finished\n",
      "2024/11/24 09:25:52 - INFO : Save mlir file: best_origin.mlir\n",
      "[Running]: tpuc-opt best_origin.mlir --shape-infer --canonicalize --extra-optimize -o best.mlir\n",
      "MulToSiLU : top.Mul succeed!\n",
      "MulToSiLU : top.Mul succeed!\n",
      "MulToSiLU : top.Mul succeed!\n",
      "MulToSiLU : top.Mul succeed!\n",
      "MulToSiLU : top.Mul succeed!\n",
      "MulToSiLU : top.Mul succeed!\n",
      "MulToSiLU : top.Mul succeed!\n",
      "MulToSiLU : top.Mul succeed!\n",
      "MulToSiLU : top.Mul succeed!\n",
      "MulToSiLU : top.Mul succeed!\n",
      "MulToSiLU : top.Mul succeed!\n",
      "MulToSiLU : top.Mul succeed!\n",
      "MulToSiLU : top.Mul succeed!\n",
      "MulToSiLU : top.Mul succeed!\n",
      "MulToSiLU : top.Mul succeed!\n",
      "MulToSiLU : top.Mul succeed!\n",
      "MulToSiLU : top.Mul succeed!\n",
      "MulToSiLU : top.Mul succeed!\n",
      "MulToSiLU : top.Mul succeed!\n",
      "MulToSiLU : top.Mul succeed!\n",
      "MulToSiLU : top.Mul succeed!\n",
      "MulToSiLU : top.Mul succeed!\n",
      "MulToSiLU : top.Mul succeed!\n",
      "MulToSiLU : top.Mul succeed!\n",
      "MulToSiLU : top.Mul succeed!\n",
      "MulToSiLU : top.Mul succeed!\n",
      "MulToSiLU : top.Mul succeed!\n",
      "MulToSiLU : top.Mul succeed!\n",
      "MulToSiLU : top.Mul succeed!\n",
      "MulToSiLU : top.Mul succeed!\n",
      "MulToSiLU : top.Mul succeed!\n",
      "MulToSiLU : top.Mul succeed!\n",
      "MulToSiLU : top.Mul succeed!\n",
      "MulToSiLU : top.Mul succeed!\n",
      "InterpToUpsampleMergePattern : top.Interp succeed!\n",
      "MulToSiLU : top.Mul succeed!\n",
      "MulToSiLU : top.Mul succeed!\n",
      "MulToSiLU : top.Mul succeed!\n",
      "MulToSiLU : top.Mul succeed!\n",
      "MulToSiLU : top.Mul succeed!\n",
      "MulToSiLU : top.Mul succeed!\n",
      "InterpToUpsampleMergePattern : top.Interp succeed!\n",
      "MulToSiLU : top.Mul succeed!\n",
      "MulToSiLU : top.Mul succeed!\n",
      "MulToSiLU : top.Mul succeed!\n",
      "MulToSiLU : top.Mul succeed!\n",
      "MulToSiLU : top.Mul succeed!\n",
      "MulToSiLU : top.Mul succeed!\n",
      "MulToSiLU : top.Mul succeed!\n",
      "MulToSiLU : top.Mul succeed!\n",
      "MulToSiLU : top.Mul succeed!\n",
      "MulToSiLU : top.Mul succeed!\n",
      "MulToSiLU : top.Mul succeed!\n",
      "MulToSiLU : top.Mul succeed!\n",
      "MulToSiLU : top.Mul succeed!\n",
      "MulToSiLU : top.Mul succeed!\n",
      "MulToSiLU : top.Mul succeed!\n",
      "MulToSiLU : top.Mul succeed!\n",
      "MulToSiLU : top.Mul succeed!\n",
      "[Success]: tpuc-opt best_origin.mlir --shape-infer --canonicalize --extra-optimize -o best.mlir\n",
      "2024/11/24 09:25:56 - INFO : Mlir file generated:best.mlir\n",
      "2024/11/24 09:25:56 - INFO : \n",
      "  load_config Preprocess args : \n",
      "\tresize_dims           : [640, 640]\n",
      "\tkeep_aspect_ratio     : True\n",
      "\tkeep_ratio_mode       : letterbox\n",
      "\tpad_value             : 0\n",
      "\tpad_type              : center\n",
      "\tinput_dims            : [640, 640]\n",
      "\t--------------------------\n",
      "\tmean                  : [0.0, 0.0, 0.0]\n",
      "\tscale                 : [0.003921569, 0.003921569, 0.003921569]\n",
      "\t--------------------------\n",
      "\tpixel_format          : rgb\n",
      "\tchannel_format        : nchw\n",
      "\n",
      "[CMD]: model_runner.py --input best_in_f32.npz --model ../best.onnx --output best_ref_outputs.npz  \n",
      "2024/11/24 09:25:58 - INFO : Saving best_ref_outputs.npz\n",
      "[CMD]: model_runner.py --input best_in_f32.npz --model best.mlir --output best_top_outputs.npz  \n",
      "[                                                  ]                                                   ]                                                   ] #                                                 ] #                                                 ] 4#                                                ] 4#                                                ] #                                               ] 8#                                              ] 10#                                             ] 10111#                                            ] 1#                                            ] 121314#                                           ] 14#                                           ] 14151#                                          ] 161718#                                         ] 18192#                                        ] 202122#                                       ] 22232#                                      ] 2#                                      ] 242526#                                     ] 26#                                     ] 26272#                                    ] 282930#                                   ] 30313#                                  ] 323334#                                 ] 34#                                 ] 34353#                                ] 3#                                ] 363738#                               ] 38394#                              ] 404142#                             ] 42434#                            ] 444546#                           ] 46#                           ] 46474#                          ] 4#                          ] 484950#                         ] 50515#                        ] 525354#                       ] 54555#                      ] 5#                      ] 565758#                     ] 58#                     ] 58596#                    ] 6#                    ] 606162#                   ] 62636#                  ] 646566#                 ] 66676#                ] 6#                ] 686970#               ] 70#               ] 70717#              ] 727374#             ] 74757#            ] 767778#           ] 78#           ] 78798#          ] 8#          ] 808182#         ] 82#         ] 82838#        ] 848586#       ] 86878#      ] 888990#     ] 90#     ] 90919#    ] 9#    ] 929394#   ] 94959#  ] 969798# ] 9899100#] 100%\n",
      "2024/11/24 09:26:01 - INFO : Saving best_top_outputs.npz\n",
      "[Running]: npz_tool.py compare best_top_outputs.npz best_ref_outputs.npz --tolerance 0.99,0.99 --except - -vv\n",
      "compare /model.24/m.2/Conv_output_0_Conv:  99%|▉| 141/142 [00:06<00:00, 29.14it/\n",
      "\n",
      "[/model.0/conv/Conv_output_0_Conv]      SIMILAR [PASSED]\n",
      "    (1, 32, 320, 320) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 1.000000\n",
      "    sqnr_similarity        = 135.577641\n",
      "[/model.0/act/Mul_output_0_Mul   ]      SIMILAR [PASSED]\n",
      "    (1, 32, 320, 320) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 1.000000\n",
      "    sqnr_similarity        = 132.169256\n",
      "[/model.1/conv/Conv_output_0_Conv]      SIMILAR [PASSED]\n",
      "    (1, 64, 160, 160) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 1.000000\n",
      "    sqnr_similarity        = 126.330080\n",
      "[/model.1/act/Mul_output_0_Mul   ]      SIMILAR [PASSED]\n",
      "    (1, 64, 160, 160) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 1.000000\n",
      "    sqnr_similarity        = 124.674559\n",
      "[/model.2/cv1/conv/Conv_output_0_Conv]      SIMILAR [PASSED]\n",
      "    (1, 32, 160, 160) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 1.000000\n",
      "    sqnr_similarity        = 128.050089\n",
      "[/model.2/cv1/act/Mul_output_0_Mul]      SIMILAR [PASSED]\n",
      "    (1, 32, 160, 160) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 1.000000\n",
      "    sqnr_similarity        = 129.341736\n",
      "[/model.2/m/m.0/cv1/conv/Conv_output_0_Conv]      SIMILAR [PASSED]\n",
      "    (1, 32, 160, 160) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 1.000000\n",
      "    sqnr_similarity        = 125.209808\n",
      "[/model.2/m/m.0/cv1/act/Mul_output_0_Mul]      SIMILAR [PASSED]\n",
      "    (1, 32, 160, 160) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 1.000000\n",
      "    sqnr_similarity        = 127.140808\n",
      "[/model.2/m/m.0/cv2/conv/Conv_output_0_Conv]      SIMILAR [PASSED]\n",
      "    (1, 32, 160, 160) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 121.456232\n",
      "[/model.2/m/m.0/cv2/act/Mul_output_0_Mul]      SIMILAR [PASSED]\n",
      "    (1, 32, 160, 160) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 121.031113\n",
      "[/model.2/m/m.0/Add_output_0_Add ]      SIMILAR [PASSED]\n",
      "    (1, 32, 160, 160) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 123.049002\n",
      "[/model.2/cv2/conv/Conv_output_0_Conv]      SIMILAR [PASSED]\n",
      "    (1, 32, 160, 160) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 1.000000\n",
      "    sqnr_similarity        = 127.573481\n",
      "[/model.2/cv2/act/Mul_output_0_Mul]      SIMILAR [PASSED]\n",
      "    (1, 32, 160, 160) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 1.000000\n",
      "    sqnr_similarity        = 126.956978\n",
      "[/model.2/Concat_output_0_Concat ]      SIMILAR [PASSED]\n",
      "    (1, 64, 160, 160) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 1.000000\n",
      "    sqnr_similarity        = 124.101734\n",
      "[/model.2/cv3/conv/Conv_output_0_Conv]      SIMILAR [PASSED]\n",
      "    (1, 64, 160, 160) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 120.992908\n",
      "[/model.2/cv3/act/Mul_output_0_Mul]      SIMILAR [PASSED]\n",
      "    (1, 64, 160, 160) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 122.028179\n",
      "[/model.3/conv/Conv_output_0_Conv]      SIMILAR [PASSED]\n",
      "    (1, 128, 80, 80) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 122.484360\n",
      "[/model.3/act/Mul_output_0_Mul   ]      SIMILAR [PASSED]\n",
      "    (1, 128, 80, 80) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 123.299007\n",
      "[/model.4/cv1/conv/Conv_output_0_Conv]      SIMILAR [PASSED]\n",
      "    (1, 64, 80, 80) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 1.000000\n",
      "    sqnr_similarity        = 126.059303\n",
      "[/model.4/cv1/act/Mul_output_0_Mul]      SIMILAR [PASSED]\n",
      "    (1, 64, 80, 80) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 1.000000\n",
      "    sqnr_similarity        = 126.040440\n",
      "[/model.4/m/m.0/cv1/conv/Conv_output_0_Conv]      SIMILAR [PASSED]\n",
      "    (1, 64, 80, 80) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 123.728046\n",
      "[/model.4/m/m.0/cv1/act/Mul_output_0_Mul]      SIMILAR [PASSED]\n",
      "    (1, 64, 80, 80) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 124.127693\n",
      "[/model.4/m/m.0/cv2/conv/Conv_output_0_Conv]      SIMILAR [PASSED]\n",
      "    (1, 64, 80, 80) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 123.075981\n",
      "[/model.4/m/m.0/cv2/act/Mul_output_0_Mul]      SIMILAR [PASSED]\n",
      "    (1, 64, 80, 80) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 123.487625\n",
      "[/model.4/m/m.0/Add_output_0_Add ]      SIMILAR [PASSED]\n",
      "    (1, 64, 80, 80) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 124.210224\n",
      "[/model.4/m/m.1/cv1/conv/Conv_output_0_Conv]      SIMILAR [PASSED]\n",
      "    (1, 64, 80, 80) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 121.839457\n",
      "[/model.4/m/m.1/cv1/act/Mul_output_0_Mul]      SIMILAR [PASSED]\n",
      "    (1, 64, 80, 80) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 121.426125\n",
      "[/model.4/m/m.1/cv2/conv/Conv_output_0_Conv]      SIMILAR [PASSED]\n",
      "    (1, 64, 80, 80) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 122.243185\n",
      "[/model.4/m/m.1/cv2/act/Mul_output_0_Mul]      SIMILAR [PASSED]\n",
      "    (1, 64, 80, 80) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 122.599154\n",
      "[/model.4/m/m.1/Add_output_0_Add ]      SIMILAR [PASSED]\n",
      "    (1, 64, 80, 80) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 123.813982\n",
      "[/model.4/cv2/conv/Conv_output_0_Conv]      SIMILAR [PASSED]\n",
      "    (1, 64, 80, 80) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 122.837629\n",
      "[/model.4/cv2/act/Mul_output_0_Mul]      SIMILAR [PASSED]\n",
      "    (1, 64, 80, 80) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 121.820116\n",
      "[/model.4/Concat_output_0_Concat ]      SIMILAR [PASSED]\n",
      "    (1, 128, 80, 80) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 123.239746\n",
      "[/model.4/cv3/conv/Conv_output_0_Conv]      SIMILAR [PASSED]\n",
      "    (1, 128, 80, 80) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 122.658081\n",
      "[/model.4/cv3/act/Mul_output_0_Mul]      SIMILAR [PASSED]\n",
      "    (1, 128, 80, 80) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 122.596016\n",
      "[/model.5/conv/Conv_output_0_Conv]      SIMILAR [PASSED]\n",
      "    (1, 256, 40, 40) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 122.816000\n",
      "[/model.5/act/Mul_output_0_Mul   ]      SIMILAR [PASSED]\n",
      "    (1, 256, 40, 40) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 121.466789\n",
      "[/model.6/cv1/conv/Conv_output_0_Conv]      SIMILAR [PASSED]\n",
      "    (1, 128, 40, 40) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 1.000000\n",
      "    sqnr_similarity        = 126.027689\n",
      "[/model.6/cv1/act/Mul_output_0_Mul]      SIMILAR [PASSED]\n",
      "    (1, 128, 40, 40) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 124.865818\n",
      "[/model.6/m/m.0/cv1/conv/Conv_output_0_Conv]      SIMILAR [PASSED]\n",
      "    (1, 128, 40, 40) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 123.328304\n",
      "[/model.6/m/m.0/cv1/act/Mul_output_0_Mul]      SIMILAR [PASSED]\n",
      "    (1, 128, 40, 40) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 123.007679\n",
      "[/model.6/m/m.0/cv2/conv/Conv_output_0_Conv]      SIMILAR [PASSED]\n",
      "    (1, 128, 40, 40) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 124.244251\n",
      "[/model.6/m/m.0/cv2/act/Mul_output_0_Mul]      SIMILAR [PASSED]\n",
      "    (1, 128, 40, 40) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 121.843214\n",
      "[/model.6/m/m.0/Add_output_0_Add ]      SIMILAR [PASSED]\n",
      "    (1, 128, 40, 40) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 122.699175\n",
      "[/model.6/m/m.1/cv1/conv/Conv_output_0_Conv]      SIMILAR [PASSED]\n",
      "    (1, 128, 40, 40) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 121.764460\n",
      "[/model.6/m/m.1/cv1/act/Mul_output_0_Mul]      SIMILAR [PASSED]\n",
      "    (1, 128, 40, 40) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 120.973520\n",
      "[/model.6/m/m.1/cv2/conv/Conv_output_0_Conv]      SIMILAR [PASSED]\n",
      "    (1, 128, 40, 40) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 122.606878\n",
      "[/model.6/m/m.1/cv2/act/Mul_output_0_Mul]      SIMILAR [PASSED]\n",
      "    (1, 128, 40, 40) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 120.737171\n",
      "[/model.6/m/m.1/Add_output_0_Add ]      SIMILAR [PASSED]\n",
      "    (1, 128, 40, 40) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 121.583519\n",
      "[/model.6/m/m.2/cv1/conv/Conv_output_0_Conv]      SIMILAR [PASSED]\n",
      "    (1, 128, 40, 40) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 121.979666\n",
      "[/model.6/m/m.2/cv1/act/Mul_output_0_Mul]      SIMILAR [PASSED]\n",
      "    (1, 128, 40, 40) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 121.610975\n",
      "[/model.6/m/m.2/cv2/conv/Conv_output_0_Conv]      SIMILAR [PASSED]\n",
      "    (1, 128, 40, 40) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 121.538076\n",
      "[/model.6/m/m.2/cv2/act/Mul_output_0_Mul]      SIMILAR [PASSED]\n",
      "    (1, 128, 40, 40) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 120.364838\n",
      "[/model.6/m/m.2/Add_output_0_Add ]      SIMILAR [PASSED]\n",
      "    (1, 128, 40, 40) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 120.942726\n",
      "[/model.6/cv2/conv/Conv_output_0_Conv]      SIMILAR [PASSED]\n",
      "    (1, 128, 40, 40) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 121.177340\n",
      "[/model.6/cv2/act/Mul_output_0_Mul]      SIMILAR [PASSED]\n",
      "    (1, 128, 40, 40) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 119.704094\n",
      "[/model.6/Concat_output_0_Concat ]      SIMILAR [PASSED]\n",
      "    (1, 256, 40, 40) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 120.691700\n",
      "[/model.6/cv3/conv/Conv_output_0_Conv]      SIMILAR [PASSED]\n",
      "    (1, 256, 40, 40) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 122.359667\n",
      "[/model.6/cv3/act/Mul_output_0_Mul]      SIMILAR [PASSED]\n",
      "    (1, 256, 40, 40) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 120.830736\n",
      "[/model.7/conv/Conv_output_0_Conv]      SIMILAR [PASSED]\n",
      "    (1, 512, 20, 20) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 121.820440\n",
      "[/model.7/act/Mul_output_0_Mul   ]      SIMILAR [PASSED]\n",
      "    (1, 512, 20, 20) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 119.770594\n",
      "[/model.8/cv1/conv/Conv_output_0_Conv]      SIMILAR [PASSED]\n",
      "    (1, 256, 20, 20) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 1.000000\n",
      "    sqnr_similarity        = 122.391567\n",
      "[/model.8/cv1/act/Mul_output_0_Mul]      SIMILAR [PASSED]\n",
      "    (1, 256, 20, 20) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 120.093288\n",
      "[/model.8/m/m.0/cv1/conv/Conv_output_0_Conv]      SIMILAR [PASSED]\n",
      "    (1, 256, 20, 20) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 118.444233\n",
      "[/model.8/m/m.0/cv1/act/Mul_output_0_Mul]      SIMILAR [PASSED]\n",
      "    (1, 256, 20, 20) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 116.929636\n",
      "[/model.8/m/m.0/cv2/conv/Conv_output_0_Conv]      SIMILAR [PASSED]\n",
      "    (1, 256, 20, 20) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 120.116892\n",
      "[/model.8/m/m.0/cv2/act/Mul_output_0_Mul]      SIMILAR [PASSED]\n",
      "    (1, 256, 20, 20) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 118.770351\n",
      "[/model.8/m/m.0/Add_output_0_Add ]      SIMILAR [PASSED]\n",
      "    (1, 256, 20, 20) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 118.792496\n",
      "[/model.8/cv2/conv/Conv_output_0_Conv]      SIMILAR [PASSED]\n",
      "    (1, 256, 20, 20) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 119.933987\n",
      "[/model.8/cv2/act/Mul_output_0_Mul]      SIMILAR [PASSED]\n",
      "    (1, 256, 20, 20) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 118.576183\n",
      "[/model.8/Concat_output_0_Concat ]      SIMILAR [PASSED]\n",
      "    (1, 512, 20, 20) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 118.736629\n",
      "[/model.8/cv3/conv/Conv_output_0_Conv]      SIMILAR [PASSED]\n",
      "    (1, 512, 20, 20) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 120.000591\n",
      "[/model.8/cv3/act/Mul_output_0_Mul]      SIMILAR [PASSED]\n",
      "    (1, 512, 20, 20) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 118.601189\n",
      "[/model.9/cv1/conv/Conv_output_0_Conv]      SIMILAR [PASSED]\n",
      "    (1, 256, 20, 20) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 121.172705\n",
      "[/model.9/cv1/act/Mul_output_0_Mul]      SIMILAR [PASSED]\n",
      "    (1, 256, 20, 20) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 120.867329\n",
      "[/model.9/m/MaxPool_output_0_MaxPool]      SIMILAR [PASSED]\n",
      "    (1, 256, 20, 20) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 120.163870\n",
      "[/model.9/m_1/MaxPool_output_0_MaxPool]      SIMILAR [PASSED]\n",
      "    (1, 256, 20, 20) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 1.000000\n",
      "    sqnr_similarity        = 119.949284\n",
      "[/model.9/m_2/MaxPool_output_0_MaxPool]      SIMILAR [PASSED]\n",
      "    (1, 256, 20, 20) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 1.000000\n",
      "    sqnr_similarity        = 119.772367\n",
      "[/model.9/Concat_output_0_Concat ]      SIMILAR [PASSED]\n",
      "    (1, 1024, 20, 20) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 1.000000\n",
      "    sqnr_similarity        = 121.441851\n",
      "[/model.9/cv2/conv/Conv_output_0_Conv]      SIMILAR [PASSED]\n",
      "    (1, 512, 20, 20) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 120.672970\n",
      "[/model.9/cv2/act/Mul_output_0_Mul]      SIMILAR [PASSED]\n",
      "    (1, 512, 20, 20) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 119.237318\n",
      "[/model.10/conv/Conv_output_0_Conv]      SIMILAR [PASSED]\n",
      "    (1, 256, 20, 20) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 120.550699\n",
      "[/model.10/act/Mul_output_0_Mul  ]      SIMILAR [PASSED]\n",
      "    (1, 256, 20, 20) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 120.662460\n",
      "[/model.11/Resize_output_0_Resize]      SIMILAR [PASSED]\n",
      "    (1, 256, 40, 40) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 120.662460\n",
      "[/model.12/Concat_output_0_Concat]      SIMILAR [PASSED]\n",
      "    (1, 512, 40, 40) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 120.795841\n",
      "[/model.13/cv1/conv/Conv_output_0_Conv]      SIMILAR [PASSED]\n",
      "    (1, 128, 40, 40) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 122.337313\n",
      "[/model.13/cv1/act/Mul_output_0_Mul]      SIMILAR [PASSED]\n",
      "    (1, 128, 40, 40) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 121.706743\n",
      "[/model.13/m/m.0/cv1/conv/Conv_output_0_Conv]      SIMILAR [PASSED]\n",
      "    (1, 128, 40, 40) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 121.704645\n",
      "[/model.13/m/m.0/cv1/act/Mul_output_0_Mul]      SIMILAR [PASSED]\n",
      "    (1, 128, 40, 40) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 121.371422\n",
      "[/model.13/m/m.0/cv2/conv/Conv_output_0_Conv]      SIMILAR [PASSED]\n",
      "    (1, 128, 40, 40) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 120.430384\n",
      "[/model.13/m/m.0/cv2/act/Mul_output_0_Mul]      SIMILAR [PASSED]\n",
      "    (1, 128, 40, 40) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 119.876518\n",
      "[/model.13/cv2/conv/Conv_output_0_Conv]      SIMILAR [PASSED]\n",
      "    (1, 128, 40, 40) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 121.005278\n",
      "[/model.13/cv2/act/Mul_output_0_Mul]      SIMILAR [PASSED]\n",
      "    (1, 128, 40, 40) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 119.496984\n",
      "[/model.13/Concat_output_0_Concat]      SIMILAR [PASSED]\n",
      "    (1, 256, 40, 40) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 119.644613\n",
      "[/model.13/cv3/conv/Conv_output_0_Conv]      SIMILAR [PASSED]\n",
      "    (1, 256, 40, 40) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 120.356016\n",
      "[/model.13/cv3/act/Mul_output_0_Mul]      SIMILAR [PASSED]\n",
      "    (1, 256, 40, 40) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 118.483009\n",
      "[/model.14/conv/Conv_output_0_Conv]      SIMILAR [PASSED]\n",
      "    (1, 128, 40, 40) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 120.453672\n",
      "[/model.14/act/Mul_output_0_Mul  ]      SIMILAR [PASSED]\n",
      "    (1, 128, 40, 40) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 120.393181\n",
      "[/model.15/Resize_output_0_Resize]      SIMILAR [PASSED]\n",
      "    (1, 128, 80, 80) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 120.393181\n",
      "[/model.16/Concat_output_0_Concat]      SIMILAR [PASSED]\n",
      "    (1, 256, 80, 80) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 121.124554\n",
      "[/model.17/cv1/conv/Conv_output_0_Conv]      SIMILAR [PASSED]\n",
      "    (1, 64, 80, 80) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 124.987488\n",
      "[/model.17/cv1/act/Mul_output_0_Mul]      SIMILAR [PASSED]\n",
      "    (1, 64, 80, 80) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 1.000000\n",
      "    sqnr_similarity        = 126.152706\n",
      "[/model.17/m/m.0/cv1/conv/Conv_output_0_Conv]      SIMILAR [PASSED]\n",
      "    (1, 64, 80, 80) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 125.241413\n",
      "[/model.17/m/m.0/cv1/act/Mul_output_0_Mul]      SIMILAR [PASSED]\n",
      "    (1, 64, 80, 80) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 125.640402\n",
      "[/model.17/m/m.0/cv2/conv/Conv_output_0_Conv]      SIMILAR [PASSED]\n",
      "    (1, 64, 80, 80) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 122.520494\n",
      "[/model.17/m/m.0/cv2/act/Mul_output_0_Mul]      SIMILAR [PASSED]\n",
      "    (1, 64, 80, 80) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 121.557636\n",
      "[/model.17/cv2/conv/Conv_output_0_Conv]      SIMILAR [PASSED]\n",
      "    (1, 64, 80, 80) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 121.075706\n",
      "[/model.17/cv2/act/Mul_output_0_Mul]      SIMILAR [PASSED]\n",
      "    (1, 64, 80, 80) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 120.533447\n",
      "[/model.17/Concat_output_0_Concat]      SIMILAR [PASSED]\n",
      "    (1, 128, 80, 80) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 121.138210\n",
      "[/model.17/cv3/conv/Conv_output_0_Conv]      SIMILAR [PASSED]\n",
      "    (1, 128, 80, 80) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 122.715092\n",
      "[/model.17/cv3/act/Mul_output_0_Mul]      SIMILAR [PASSED]\n",
      "    (1, 128, 80, 80) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 121.195307\n",
      "[/model.18/conv/Conv_output_0_Conv]      SIMILAR [PASSED]\n",
      "    (1, 128, 40, 40) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 121.096525\n",
      "[/model.18/act/Mul_output_0_Mul  ]      SIMILAR [PASSED]\n",
      "    (1, 128, 40, 40) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 119.438553\n",
      "[/model.19/Concat_output_0_Concat]      SIMILAR [PASSED]\n",
      "    (1, 256, 40, 40) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 119.957008\n",
      "[/model.20/cv1/conv/Conv_output_0_Conv]      SIMILAR [PASSED]\n",
      "    (1, 128, 40, 40) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 119.819975\n",
      "[/model.20/cv1/act/Mul_output_0_Mul]      SIMILAR [PASSED]\n",
      "    (1, 128, 40, 40) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 118.669920\n",
      "[/model.20/m/m.0/cv1/conv/Conv_output_0_Conv]      SIMILAR [PASSED]\n",
      "    (1, 128, 40, 40) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 119.501619\n",
      "[/model.20/m/m.0/cv1/act/Mul_output_0_Mul]      SIMILAR [PASSED]\n",
      "    (1, 128, 40, 40) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 117.932491\n",
      "[/model.20/m/m.0/cv2/conv/Conv_output_0_Conv]      SIMILAR [PASSED]\n",
      "    (1, 128, 40, 40) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 119.477997\n",
      "[/model.20/m/m.0/cv2/act/Mul_output_0_Mul]      SIMILAR [PASSED]\n",
      "    (1, 128, 40, 40) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 118.626184\n",
      "[/model.20/cv2/conv/Conv_output_0_Conv]      SIMILAR [PASSED]\n",
      "    (1, 128, 40, 40) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 122.094831\n",
      "[/model.20/cv2/act/Mul_output_0_Mul]      SIMILAR [PASSED]\n",
      "    (1, 128, 40, 40) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 121.292257\n",
      "[/model.20/Concat_output_0_Concat]      SIMILAR [PASSED]\n",
      "    (1, 256, 40, 40) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 119.560747\n",
      "[/model.20/cv3/conv/Conv_output_0_Conv]      SIMILAR [PASSED]\n",
      "    (1, 256, 40, 40) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 121.885195\n",
      "[/model.20/cv3/act/Mul_output_0_Mul]      SIMILAR [PASSED]\n",
      "    (1, 256, 40, 40) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 120.447102\n",
      "[/model.21/conv/Conv_output_0_Conv]      SIMILAR [PASSED]\n",
      "    (1, 256, 20, 20) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 120.929842\n",
      "[/model.21/act/Mul_output_0_Mul  ]      SIMILAR [PASSED]\n",
      "    (1, 256, 20, 20) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 119.482508\n",
      "[/model.22/Concat_output_0_Concat]      SIMILAR [PASSED]\n",
      "    (1, 512, 20, 20) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 120.264273\n",
      "[/model.23/cv1/conv/Conv_output_0_Conv]      SIMILAR [PASSED]\n",
      "    (1, 256, 20, 20) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 121.209269\n",
      "[/model.23/cv1/act/Mul_output_0_Mul]      SIMILAR [PASSED]\n",
      "    (1, 256, 20, 20) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 119.868832\n",
      "[/model.23/m/m.0/cv1/conv/Conv_output_0_Conv]      SIMILAR [PASSED]\n",
      "    (1, 256, 20, 20) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 120.135822\n",
      "[/model.23/m/m.0/cv1/act/Mul_output_0_Mul]      SIMILAR [PASSED]\n",
      "    (1, 256, 20, 20) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 118.809843\n",
      "[/model.23/m/m.0/cv2/conv/Conv_output_0_Conv]      SIMILAR [PASSED]\n",
      "    (1, 256, 20, 20) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 120.914440\n",
      "[/model.23/m/m.0/cv2/act/Mul_output_0_Mul]      SIMILAR [PASSED]\n",
      "    (1, 256, 20, 20) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 118.993959\n",
      "[/model.23/cv2/conv/Conv_output_0_Conv]      SIMILAR [PASSED]\n",
      "    (1, 256, 20, 20) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 121.952820\n",
      "[/model.23/cv2/act/Mul_output_0_Mul]      SIMILAR [PASSED]\n",
      "    (1, 256, 20, 20) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 121.240387\n",
      "[/model.23/Concat_output_0_Concat]      SIMILAR [PASSED]\n",
      "    (1, 512, 20, 20) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 119.858036\n",
      "[/model.23/cv3/conv/Conv_output_0_Conv]      SIMILAR [PASSED]\n",
      "    (1, 512, 20, 20) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 122.739210\n",
      "[/model.23/cv3/act/Mul_output_0_Mul]      SIMILAR [PASSED]\n",
      "    (1, 512, 20, 20) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 0.999999\n",
      "    sqnr_similarity        = 122.080193\n",
      "[/model.24/m.0/Conv_output_0_Conv]      SIMILAR [PASSED]\n",
      "    (1, 18, 80, 80) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 1.000000\n",
      "    sqnr_similarity        = 134.848032\n",
      "[/model.24/m.1/Conv_output_0_Conv]      SIMILAR [PASSED]\n",
      "    (1, 18, 40, 40) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 1.000000\n",
      "    sqnr_similarity        = 134.600945\n",
      "[/model.24/m.2/Conv_output_0_Conv]      SIMILAR [PASSED]\n",
      "    (1, 18, 20, 20) float32 \n",
      "    cosine_similarity      = 1.000000\n",
      "    euclidean_similarity   = 1.000000\n",
      "    sqnr_similarity        = 135.792351\n",
      "142 compared\n",
      "142 passed\n",
      "  0 equal, 0 close, 142 similar\n",
      "0 failed\n",
      "  0 not equal, 0 not similar\n",
      "min_similiarity = (0.9999997615814209, 0.9999985794345131, 116.92963600158691)\n",
      "Target    best_top_outputs.npz\n",
      "Reference best_ref_outputs.npz\n",
      "npz compare PASSED.\n",
      "compare /model.24/m.2/Conv_output_0_Conv: 100%|█| 142/142 [00:09<00:00, 14.92it/\n",
      "[Success]: npz_tool.py compare best_top_outputs.npz best_ref_outputs.npz --tolerance 0.99,0.99 --except - -vv\n",
      "2024/11/24 09:26:14 - INFO : TPU-MLIR v1.12-20241105\n",
      "[Running]: tpuc-opt best.mlir --processor-assign=\"chip=cv181x mode=BF16 num_device=1 num_core=1 addr_mode=auto\" --processor-top-optimize --convert-top-to-tpu=\" asymmetric=False doWinograd=False ignore_f16_overflow=False q_group_size=0 matmul_perchannel=False\" --canonicalize --weight-fold -o best_cv181x_bf16_tpu.mlir\n",
      "ConvertUpsampleOp : top.Upsample succeed!\n",
      "ConvertUpsampleOp : top.Upsample succeed!\n",
      "[Success]: tpuc-opt best.mlir --processor-assign=\"chip=cv181x mode=BF16 num_device=1 num_core=1 addr_mode=auto\" --processor-top-optimize --convert-top-to-tpu=\" asymmetric=False doWinograd=False ignore_f16_overflow=False q_group_size=0 matmul_perchannel=False\" --canonicalize --weight-fold -o best_cv181x_bf16_tpu.mlir\n",
      "[CMD]: model_runner.py --input best_in_f32.npz --model best_cv181x_bf16_tpu.mlir --output best_cv181x_bf16_tpu_outputs.npz  \n",
      "[                                                  ]                                                   ]                                                   ] #                                                 ] #                                                 ] 4#                                                ] 4#                                                ] #                                               ] #                                               ] 8#                                              ] 8#                                              ] 10#                                             ] 10111#                                            ] 121314#                                           ] 14151#                                          ] 161718#                                         ] 18192#                                        ] 2#                                        ] 202122#                                       ] 22#                                       ] 22232#                                      ] 2#                                      ] 242526#                                     ] 26#                                     ] 26272#                                    ] 2#                                    ] 282930#                                   ] 30313#                                  ] 323334#                                 ] 34353#                                ] 363738#                               ] 38394#                              ] 4#                              ] 404142#                             ] 42#                             ] 42434#                            ] 4#                            ] 444546#                           ] 46#                           ] 46474#                          ] 4#                          ] 484950#                         ] 50515#                        ] 525354#                       ] 54555#                      ] 565758#                     ] 58596#                    ] 6#                    ] 606162#                   ] 62#                   ] 62636#                  ] 6#                  ] 646566#                 ] 66#                 ] 66676#                ] 6#                ] 686970#               ] 70717#              ] 727374#             ] 74757#            ] 767778#           ] 78798#          ] 8#          ] 808182#         ] 82#         ] 82838#        ] 8#        ] 848586#       ] 86#       ] 86878#      ] 8#      ] 888990#     ] 90919#    ] 929394#   ] 94959#  ] 969798# ] 9899100#] 100%\n",
      "[Running]: npz_tool.py compare best_cv181x_bf16_tpu_outputs.npz best_top_outputs.npz --tolerance 0.8,0.5 --except - -vv\n",
      "compare /model.24/m.2/Conv_output_0_Conv:  67%|█▎| 2/3 [00:00<00:00, 477.63it/s]\n",
      "\n",
      "[/model.24/m.0/Conv_output_0_Conv]      SIMILAR [PASSED]\n",
      "    (1, 18, 80, 80) float32 \n",
      "    cosine_similarity      = 0.999996\n",
      "    euclidean_similarity   = 0.995681\n",
      "    sqnr_similarity        = 47.317729\n",
      "[/model.24/m.1/Conv_output_0_Conv]      SIMILAR [PASSED]\n",
      "    (1, 18, 40, 40) float32 \n",
      "    cosine_similarity      = 0.999989\n",
      "    euclidean_similarity   = 0.994947\n",
      "    sqnr_similarity        = 46.122751\n",
      "[/model.24/m.2/Conv_output_0_Conv]      SIMILAR [PASSED]\n",
      "    (1, 18, 20, 20) float32 \n",
      "    cosine_similarity      = 0.999986\n",
      "    euclidean_similarity   = 0.994618\n",
      "    sqnr_similarity        = 45.462565\n",
      "3 compared\n",
      "3 passed\n",
      "  0 equal, 0 close, 3 similar\n",
      "0 failed\n",
      "  0 not equal, 0 not similar\n",
      "min_similiarity = (0.999986469745636, 0.994617628360038, 45.462565422058105)\n",
      "Target    best_cv181x_bf16_tpu_outputs.npz\n",
      "Reference best_top_outputs.npz\n",
      "npz compare PASSED.\n",
      "compare /model.24/m.2/Conv_output_0_Conv: 100%|███| 3/3 [00:00<00:00, 31.92it/s]\n",
      "[Success]: npz_tool.py compare best_cv181x_bf16_tpu_outputs.npz best_top_outputs.npz --tolerance 0.8,0.5 --except - -vv\n",
      "[Running]: tpuc-opt best_cv181x_bf16_tpu.mlir --mlir-disable-threading --strip-io-quant=\"quant_input=False quant_output=False quant_input_list= quant_output_list=\" --processor-tpu-optimize --dev-parallel --weight-reorder  --subnet-divide=\"dynamic=False\" --op-reorder --future-update=\"rank=0 weight_list=\" --layer-group=\"opt=2 group_by_cores=auto compress_mode=none\" --core-parallel --address-assign -o best_cv181x_bf16_final.mlir \n",
      "==---------------------------==\n",
      "Run LayerGroupSearchPass : \n",
      "    Searching the optimal layer groups\n",
      "==---------------------------==\n",
      "\n",
      "=======================================================\n",
      "***** Dynamic Programming layer group with cluster ****\n",
      "=======================================================\n",
      "total num of base_group is 6\n",
      "clusters idx(size): 0(1), \n",
      "process base group 0, layer_num=1, cluster_num=1\n",
      "clusters idx(size): 0(1), 1(1), 2(1), 3(1), 4(2), 6(2), 8(2), 10(2), 12(1), 13(1), 14(2), 16(1), 17(2), 19(2), 21(1), 22(1), 23(2), 25(2), 27(1), 28(2), 30(2), 32(1), 33(1), 34(1), 35(1), 36(2), 38(1), 39(1), 40(1), 41(1), 42(2), 44(1), 45(1), 46(1), 47(2), 49(1), 50(1), 51(1), 52(2), 54(1), 55(1), 56(1), 57(1), 58(1), 59(1), 60(1), 61(1), 62(1), 63(1), 64(1), 65(1), 66(2), 68(1), 69(1), 70(1), 71(1), 72(1), 73(1), 74(1), 75(1), 76(1), 77(1), 78(1), 79(1), 80(1), 81(1), 82(1), 83(1), 84(1), 85(1), 86(1), 87(1), 88(1), 89(1), 90(1), 91(1), 92(1), 93(1), 94(1), 95(2), 97(1), 98(1), 99(1), 100(2), 102(2), 104(1), 105(2), 107(1), 108(1), 109(1), 110(1), 111(1), 112(1), 113(1), 114(1), 115(1), 116(1), 117(1), 118(1), 119(2), 121(1), 122(1), 123(1), 124(1), 125(1), 126(1), 127(1), 128(1), 129(1), 130(1), 131(1), 132(1), 133(1), 134(1), 135(1), 136(1), 137(1), 138(1), 139(1), 140(2), \n",
      "process base group 1, layer_num=142, cluster_num=120\n",
      "Searching best group slices...\n",
      "[                                                  ]                                                   ]                                                   ] #                                                 ] 4#                                                ] #                                               ] 8#                                              ] 10#                                             ] 10111#                                            ] 121314#                                           ] 14151#                                          ] 1#                                          ] 161718#                                         ] 18192#                                        ] 202122#                                       ] 22#                                       ] 22232#                                      ] 242526#                                     ] 26272#                                    ] 282930#                                   ] 30313#                                  ] 323334#                                 ] 34353#                                ] 363738#                               ] 38#                               ] 38394#                              ] 404142#                             ] 42434#                            ] 4#                            ] 444546#                           ] 46474#                          ] 484950#                         ] 50#                         ] 50515#                        ] 525354#                       ] 54555#                      ] 565758#                     ] 58596#                    ] 606162#                   ] 62636#                  ] 646566#                 ] 66#                 ] 66676#                ] 686970#               ] 70717#              ] 7#              ] 727374#             ] 74757#            ] 767778#           ] 78798#          ] 808182#         ] 82838#        ] 848586#       ] 86878#      ] 8#      ] 888990#     ] 90919#    ] 929394#   ] 94#   ] 94959#  ] 969798# ] 9899100#] 100%\n",
      "clusters idx(size): 0(1), \n",
      "process base group 2, layer_num=1, cluster_num=1\n",
      "clusters idx(size): 0(1), \n",
      "process base group 3, layer_num=1, cluster_num=1\n",
      "clusters idx(size): 0(1), \n",
      "process base group 4, layer_num=1, cluster_num=1\n",
      "clusters idx(size): 0(1), \n",
      "process base group 5, layer_num=1, cluster_num=1\n",
      "-------------------------------------------------------\n",
      "Consider redundant computation and gdma cost\n",
      "-------------------------------------------------------\n",
      "The final cost of the two group is 610909\n",
      "//// Group cost 610909, optimal cut idx 140\n",
      "The final cost of the two group is 602072\n",
      "//// Group cost 602072, optimal cut idx 139\n",
      "The final cost of the two group is 858289\n",
      "//// Group cost 858289, optimal cut idx 138\n",
      "The final cost of the two group is 2407306\n",
      "//// Group cost 2407306, optimal cut idx 137\n",
      "The final cost of the two group is 1896448\n",
      "//// Group cost 1896448, optimal cut idx 136\n",
      "The final cost of the two group is 255747\n",
      "//// Group cost 255747, optimal cut idx 135\n",
      "The final cost of the two group is 1205891\n",
      "//// Group cost 1205891, optimal cut idx 134\n",
      "The final cost of the two group is 1205891\n",
      "//// Group cost 1205891, optimal cut idx 133\n",
      "The final cost of the two group is 4305347\n",
      "//// Group cost 4305347, optimal cut idx 132\n",
      "The final cost of the two group is 4305347\n",
      "//// Group cost 4305347, optimal cut idx 131\n",
      "The final cost of the two group is 692209\n",
      "//// Group cost 692209, optimal cut idx 130\n",
      "The final cost of the two group is 692209\n",
      "//// Group cost 692209, optimal cut idx 129\n",
      "The final cost of the two group is 1205891\n",
      "//// Group cost 1205891, optimal cut idx 128\n",
      "The final cost of the two group is 950144\n",
      "//// Group cost 950144, optimal cut idx 127\n",
      "The final cost of the two group is 255747\n",
      "//// Group cost 255747, optimal cut idx 126\n",
      "The final cost of the two group is 6443747\n",
      "//// Group cost 6443747, optimal cut idx 125\n",
      "The final cost of the two group is 7209922\n",
      "//// Group cost 7209922, optimal cut idx 124\n",
      "The final cost of the two group is 2760738\n",
      "//// Group cost 2760738, optimal cut idx 123\n",
      "The final cost of the two group is 1738816\n",
      "//// Group cost 1738816, optimal cut idx 122\n",
      "The final cost of the two group is 1741774\n",
      "//// Group cost 1741774, optimal cut idx 121\n",
      "The final cost of the two group is 1898652\n",
      "The final cost of the two group is 1911523\n",
      "The final cost of the two group is 5478574\n",
      "//// Group cost 5478574, optimal cut idx 118\n",
      "The final cost of the two group is 5910065\n",
      "//// Group cost 5910065, optimal cut idx 117\n",
      "The final cost of the two group is 2341738\n",
      "//// Group cost 2341738, optimal cut idx 116\n",
      "The final cost of the two group is 2374877\n",
      "The final cost of the two group is 2522398\n",
      "The final cost of the two group is 2173265\n",
      "//// Group cost 2173265, optimal cut idx 113\n",
      "The final cost of the two group is 510858\n",
      "//// Group cost 510858, optimal cut idx 112\n",
      "The final cost of the two group is 4322858\n",
      "//// Group cost 4322858, optimal cut idx 111\n",
      "The final cost of the two group is 7529600\n",
      "//// Group cost 7529600, optimal cut idx 110\n",
      "The final cost of the two group is 7200694\n",
      "//// Group cost 7200694, optimal cut idx 107\n",
      "The final cost of the two group is 7244134\n",
      "//// Group cost 7244134, optimal cut idx 104\n",
      "The final cost of the two group is 5102080\n",
      "//// Group cost 5102080, optimal cut idx 103\n",
      "The final cost of the two group is 3923952\n",
      "//// Group cost 3923952, optimal cut idx 101\n",
      "The final cost of the two group is 3987926\n",
      "The final cost of the two group is 3138082\n",
      "//// Group cost 3138082, optimal cut idx 100\n",
      "The final cost of the two group is 2582912\n",
      "//// Group cost 2582912, optimal cut idx 99\n",
      "The final cost of the two group is 323280\n",
      "//// Group cost 323280, optimal cut idx 98\n",
      "The final cost of the two group is 834138\n",
      "//// Group cost 834138, optimal cut idx 97\n",
      "The final cost of the two group is 2242192\n",
      "//// Group cost 2242192, optimal cut idx 96\n",
      "The final cost of the two group is 2456087\n",
      "The final cost of the two group is 3470150\n",
      "//// Group cost 3470150, optimal cut idx 94\n",
      "The final cost of the two group is 1738816\n",
      "//// Group cost 1738816, optimal cut idx 93\n",
      "The final cost of the two group is 510858\n",
      "//// Group cost 510858, optimal cut idx 92\n",
      "The final cost of the two group is 2225098\n",
      "//// Group cost 2225098, optimal cut idx 91\n",
      "The final cost of the two group is 2225098\n",
      "//// Group cost 2225098, optimal cut idx 90\n",
      "The final cost of the two group is 4247658\n",
      "//// Group cost 4247658, optimal cut idx 89\n",
      "The final cost of the two group is 5203970\n",
      "//// Group cost 5203970, optimal cut idx 88\n",
      "The final cost of the two group is 3181410\n",
      "//// Group cost 3181410, optimal cut idx 85\n",
      "The final cost of the two group is 1714240\n",
      "//// Group cost 1714240, optimal cut idx 84\n",
      "The final cost of the two group is 162160\n",
      "//// Group cost 162160, optimal cut idx 83\n",
      "The final cost of the two group is 417907\n",
      "//// Group cost 417907, optimal cut idx 82\n",
      "The final cost of the two group is 1205891\n",
      "//// Group cost 1205891, optimal cut idx 81\n",
      "The final cost of the two group is 1461002\n",
      "//// Group cost 1461002, optimal cut idx 80\n",
      "The final cost of the two group is 4602058\n",
      "//// Group cost 4602058, optimal cut idx 79\n",
      "The final cost of the two group is 4091200\n",
      "//// Group cost 4091200, optimal cut idx 78\n",
      "The final cost of the two group is 109765\n",
      "//// Group cost 109765, optimal cut idx 77\n",
      "The final cost of the two group is 219530\n",
      "//// Group cost 219530, optimal cut idx 76\n",
      "The final cost of the two group is 219530\n",
      "//// Group cost 219530, optimal cut idx 75\n",
      "The final cost of the two group is 365512\n",
      "//// Group cost 365512, optimal cut idx 74\n",
      "The final cost of the two group is 1205891\n",
      "//// Group cost 1205891, optimal cut idx 73\n",
      "The final cost of the two group is 1461002\n",
      "//// Group cost 1461002, optimal cut idx 72\n",
      "The final cost of the two group is 2407306\n",
      "//// Group cost 2407306, optimal cut idx 71\n",
      "The final cost of the two group is 1896448\n",
      "//// Group cost 1896448, optimal cut idx 70\n",
      "The final cost of the two group is 255747\n",
      "//// Group cost 255747, optimal cut idx 69\n",
      "The final cost of the two group is 1205891\n",
      "//// Group cost 1205891, optimal cut idx 68\n",
      "The final cost of the two group is 1242670\n",
      "//// Group cost 1242670, optimal cut idx 67\n",
      "The final cost of the two group is 4342126\n",
      "//// Group cost 4342126, optimal cut idx 65\n",
      "The final cost of the two group is 4305347\n",
      "//// Group cost 4305347, optimal cut idx 64\n",
      "The final cost of the two group is 692209\n",
      "//// Group cost 692209, optimal cut idx 63\n",
      "The final cost of the two group is 692209\n",
      "//// Group cost 692209, optimal cut idx 62\n",
      "The final cost of the two group is 1205891\n",
      "//// Group cost 1205891, optimal cut idx 61\n",
      "The final cost of the two group is 1461002\n",
      "//// Group cost 1461002, optimal cut idx 60\n",
      "The final cost of the two group is 12877418\n",
      "//// Group cost 12877418, optimal cut idx 59\n",
      "The final cost of the two group is 13388482\n",
      "//// Group cost 13388482, optimal cut idx 58\n",
      "The final cost of the two group is 2760738\n",
      "//// Group cost 2760738, optimal cut idx 57\n",
      "The final cost of the two group is 1738816\n",
      "//// Group cost 1738816, optimal cut idx 56\n",
      "The final cost of the two group is 1804014\n",
      "//// Group cost 1804014, optimal cut idx 55\n",
      "The final cost of the two group is 1960892\n",
      "The final cost of the two group is 1962119\n",
      "The final cost of the two group is 1970018\n",
      "The final cost of the two group is 5540814\n",
      "//// Group cost 5540814, optimal cut idx 51\n",
      "The final cost of the two group is 4247658\n",
      "//// Group cost 4247658, optimal cut idx 50\n",
      "The final cost of the two group is 968946\n",
      "//// Group cost 968946, optimal cut idx 49\n",
      "The final cost of the two group is 1019542\n",
      "//// Group cost 1019542, optimal cut idx 48\n",
      "The final cost of the two group is 1058166\n",
      "The final cost of the two group is 4298254\n",
      "//// Group cost 4298254, optimal cut idx 46\n",
      "The final cost of the two group is 4247658\n",
      "//// Group cost 4247658, optimal cut idx 45\n",
      "The final cost of the two group is 968946\n",
      "//// Group cost 968946, optimal cut idx 44\n",
      "The final cost of the two group is 1019542\n",
      "//// Group cost 1019542, optimal cut idx 43\n",
      "The final cost of the two group is 1058166\n",
      "The final cost of the two group is 4298254\n",
      "//// Group cost 4298254, optimal cut idx 41\n",
      "The final cost of the two group is 5203970\n",
      "//// Group cost 5203970, optimal cut idx 40\n",
      "The final cost of the two group is 3198504\n",
      "//// Group cost 3198504, optimal cut idx 37\n",
      "The final cost of the two group is 3195187\n",
      "//// Group cost 3195187, optimal cut idx 36\n",
      "The final cost of the two group is 8630082\n",
      "//// Group cost 8630082, optimal cut idx 35\n",
      "The final cost of the two group is 11325760\n",
      "//// Group cost 11325760, optimal cut idx 34\n",
      "The final cost of the two group is 6768453\n",
      "//// Group cost 6768453, optimal cut idx 33\n",
      "The final cost of the two group is 6523244\n",
      "//// Group cost 6523244, optimal cut idx 31\n",
      "The final cost of the two group is 6566684\n",
      "//// Group cost 6566684, optimal cut idx 27\n",
      "The final cost of the two group is 5102080\n",
      "//// Group cost 5102080, optimal cut idx 26\n",
      "The final cost of the two group is 2507914\n",
      "//// Group cost 2507914, optimal cut idx 25\n",
      "The final cost of the two group is 2370570\n",
      "//// Group cost 2370570, optimal cut idx 24\n",
      "The final cost of the two group is 2501540\n",
      "The final cost of the two group is 4790570\n",
      "//// Group cost 4790570, optimal cut idx 22\n",
      "The final cost of the two group is 5992334\n",
      "//// Group cost 5992334, optimal cut idx 21\n",
      "The final cost of the two group is 4884694\n",
      "//// Group cost 4884694, optimal cut idx 19\n",
      "The final cost of the two group is 4874148\n",
      "//// Group cost 4874148, optimal cut idx 18\n",
      "The final cost of the two group is 5181731\n",
      "The final cost of the two group is 10220294\n",
      "//// Group cost 10220294, optimal cut idx 16\n",
      "The final cost of the two group is 12939440\n",
      "//// Group cost 12939440, optimal cut idx 15\n",
      "The final cost of the two group is 6346712\n",
      "//// Group cost 6346712, optimal cut idx 14\n",
      "The final cost of the two group is 5362000\n",
      "//// Group cost 5362000, optimal cut idx 13\n",
      "The final cost of the two group is 2958516\n",
      "//// Group cost 2958516, optimal cut idx 12\n",
      "The final cost of the two group is 3399146\n",
      "The final cost of the two group is 3668830\n",
      "The final cost of the two group is 8507308\n",
      "//// Group cost 8507308, optimal cut idx 9\n",
      "The final cost of the two group is 8599394\n",
      "The final cost of the two group is 7820496\n",
      "//// Group cost 7820496, optimal cut idx 7\n",
      "The final cost of the two group is 5215639\n",
      "//// Group cost 5215639, optimal cut idx 6\n",
      "The final cost of the two group is 4995252\n",
      "//// Group cost 4995252, optimal cut idx 5\n",
      "The final cost of the two group is 5017590\n",
      "The final cost of the two group is 6809400\n",
      "//// Group cost 6809400, optimal cut idx 3\n",
      "The final cost of the two group is 11825852\n",
      "//// Group cost 11825852, optimal cut idx 2\n",
      "The final cost of the two group is 15911475\n",
      "//// Group cost 15911475, optimal cut idx 1\n",
      "The final cost of the two group is 14212767\n",
      "//// Group cost 14212767, optimal cut idx 0\n",
      "-------------------------------------------------------\n",
      "Merge cut idx to reduce gdma cost\n",
      "-------------------------------------------------------\n",
      "GroupMethod_process time:1760220\n",
      "==---------------------------==\n",
      "Run GroupPostTransformPass : \n",
      "    Some transform after layer groups is determined\n",
      "==---------------------------==\n",
      "==---------------------------==\n",
      "Run TimeStepAssignmentPass : \n",
      "    Assign timestep task for each group.\n",
      "==---------------------------==\n",
      "==---------------------------==\n",
      "Run LocalMemoryAllocationPass : \n",
      "    Allocate local memory for all layer groups\n",
      "==---------------------------==\n",
      "==---------------------------==\n",
      "Run TimeStepCombinePass : \n",
      "    Combine time step for better parallel balance\n",
      "==---------------------------==\n",
      "==---------------------------==\n",
      "Run GroupDataMoveOverlapPass : \n",
      "    Overlap data move between two layer group\n",
      "==---------------------------==\n",
      "GmemAllocator use FitFirstAssign\n",
      "[Success]: tpuc-opt best_cv181x_bf16_tpu.mlir --mlir-disable-threading --strip-io-quant=\"quant_input=False quant_output=False quant_input_list= quant_output_list=\" --processor-tpu-optimize --dev-parallel --weight-reorder  --subnet-divide=\"dynamic=False\" --op-reorder --future-update=\"rank=0 weight_list=\" --layer-group=\"opt=2 group_by_cores=auto compress_mode=none\" --core-parallel --address-assign -o best_cv181x_bf16_final.mlir \n",
      "[Running]: tpuc-opt best_cv181x_bf16_final.mlir --codegen=\"model_file=best_bf16.cvimodel embed_debug_info=False model_version=latest bmodel_only=False\" -o /dev/null\n",
      "[Success]: tpuc-opt best_cv181x_bf16_final.mlir --codegen=\"model_file=best_bf16.cvimodel embed_debug_info=False model_version=latest bmodel_only=False\" -o /dev/null\n",
      "[CMD]: model_runner.py --input best_in_f32.npz --model best_bf16.cvimodel --output best_cv181x_bf16_model_outputs.npz  \n",
      "setenv:cv181x\n",
      "Start TPU Simulator for cv181x\n",
      "device[0] opened, 4294967296\n",
      "version: 1.4.0\n",
      "best Build at 2024-11-24 09:26:33 For platform cv181x\n",
      "Cmodel: bm_load_cmdbuf\n",
      "Max SharedMem size:13107200\n",
      "Cmodel: bm_run_cmdbuf\n",
      "device[0] closed\n",
      "[Running]: npz_tool.py compare best_cv181x_bf16_model_outputs.npz best_cv181x_bf16_tpu_outputs.npz --tolerance 0.99,0.90 --except - -vv\n",
      "compare /model.24/m.2/Conv_output_0_Conv_f32:  67%|▋| 2/3 [00:00<00:00, 443.56it\n",
      "\n",
      "[/model.24/m.0/Conv_output_0_Conv_f32]      SIMILAR [PASSED]\n",
      "    (1, 18, 80, 80) float32 \n",
      "    cosine_similarity      = 0.999997\n",
      "    euclidean_similarity   = 0.997135\n",
      "    sqnr_similarity        = 50.869169\n",
      "[/model.24/m.1/Conv_output_0_Conv_f32]      SIMILAR [PASSED]\n",
      "    (1, 18, 40, 40) float32 \n",
      "    cosine_similarity      = 0.999996\n",
      "    euclidean_similarity   = 0.997258\n",
      "    sqnr_similarity        = 51.238804\n",
      "[/model.24/m.2/Conv_output_0_Conv_f32]      SIMILAR [PASSED]\n",
      "    (1, 18, 20, 20) float32 \n",
      "    cosine_similarity      = 0.999997\n",
      "    euclidean_similarity   = 0.997674\n",
      "    sqnr_similarity        = 52.654853\n",
      "3 compared\n",
      "3 passed\n",
      "  0 equal, 0 close, 3 similar\n",
      "0 failed\n",
      "  0 not equal, 0 not similar\n",
      "min_similiarity = (0.9999964833259583, 0.9971351591480045, 50.86916923522949)\n",
      "Target    best_cv181x_bf16_model_outputs.npz\n",
      "Reference best_cv181x_bf16_tpu_outputs.npz\n",
      "npz compare PASSED.\n",
      "compare /model.24/m.2/Conv_output_0_Conv_f32: 100%|█| 3/3 [00:00<00:00, 31.40it/\n",
      "[Success]: npz_tool.py compare best_cv181x_bf16_model_outputs.npz best_cv181x_bf16_tpu_outputs.npz --tolerance 0.99,0.90 --except - -vv\n",
      "calibrate for int8 model\n",
      "TPU-MLIR v1.12-20241105\n",
      "2024/11/24 09:27:09 - INFO : \n",
      "  load_config Preprocess args : \n",
      "\tresize_dims           : [640, 640]\n",
      "\tkeep_aspect_ratio     : True\n",
      "\tkeep_ratio_mode       : letterbox\n",
      "\tpad_value             : 0\n",
      "\tpad_type              : center\n",
      "\tinput_dims            : [640, 640]\n",
      "\t--------------------------\n",
      "\tmean                  : [0.0, 0.0, 0.0]\n",
      "\tscale                 : [0.003921569, 0.003921569, 0.003921569]\n",
      "\t--------------------------\n",
      "\tpixel_format          : rgb\n",
      "\tchannel_format        : nchw\n",
      "\n",
      "last input data (idx=20) not valid, droped\n",
      "input_num = 20, ref = 20\n",
      "real input_num = 20\n",
      "activation_collect_and_calc_th for sample: 0:   0%|      | 0/20 [00:00<?, ?it/s][                                                  ]                                                   ]                                                   ] #                                                 ] #                                                 ] 4#                                                ] 4#                                                ] #                                               ] 8#                                              ] 10#                                             ] 10111#                                            ] 1#                                            ] 121314#                                           ] 14#                                           ] 14151#                                          ] 161718#                                         ] 18192#                                        ] 202122#                                       ] 22232#                                      ] 2#                                      ] 242526#                                     ] 26#                                     ] 26272#                                    ] 282930#                                   ] 30313#                                  ] 323334#                                 ] 34#                                 ] 34353#                                ] 3#                                ] 363738#                               ] 38394#                              ] 404142#                             ] 42434#                            ] 444546#                           ] 46#                           ] 46474#                          ] 4#                          ] 484950#                         ] 50515#                        ] 525354#                       ] 54555#                      ] 5#                      ] 565758#                     ] 58#                     ] 58596#                    ] 6#                    ] 606162#                   ] 62636#                  ] 646566#                 ] 66676#                ] 6#                ] 686970#               ] 70#               ] 70717#              ] 727374#             ] 74757#            ] 767778#           ] 78#           ] 78798#          ] 8#          ] 808182#         ] 82#         ] 82838#        ] 848586#       ] 86878#      ] 888990#     ] 90#     ] 90919#    ] 9#    ] 929394#   ] 94959#  ] 969798# ] 9899100#] 100%\n",
      "activation_collect_and_calc_th for sample: 1:  10%| | 2/20 [00:02<00:18,  1.03s/[                                                  ]                                                   ]                                                   ] #                                                 ] #                                                 ] 4#                                                ] 4#                                                ] #                                               ] 8#                                              ] 10#                                             ] 10111#                                            ] 1#                                            ] 121314#                                           ] 14#                                           ] 14151#                                          ] 161718#                                         ] 18192#                                        ] 202122#                                       ] 22232#                                      ] 2#                                      ] 242526#                                     ] 26#                                     ] 26272#                                    ] 282930#                                   ] 30313#                                  ] 323334#                                 ] 34#                                 ] 34353#                                ] 3#                                ] 363738#                               ] 38394#                              ] 404142#                             ] 42434#                            ] 444546#                           ] 46#                           ] 46474#                          ] 4#                          ] 484950#                         ] 50515#                        ] 525354#                       ] 54555#                      ] 5#                      ] 565758#                     ] 58#                     ] 58596#                    ] 6#                    ] 606162#                   ] 62636#                  ] 646566#                 ] 66676#                ] 6#                ] 686970#               ] 70#               ] 70717#              ] 727374#             ] 74757#            ] 767778#           ] 78#           ] 78798#          ] 8#          ] 808182#         ] 82#         ] 82838#        ] 848586#       ] 86878#      ] 888990#     ] 90#     ] 90919#    ] 9#    ] 929394#   ] 94959#  ] 969798# ] 9899100#] 100%\n",
      "activation_collect_and_calc_th for sample: 2:  15%|▏| 3/20 [00:04<00:26,  1.58s/[                                                  ]                                                   ]                                                   ] #                                                 ] #                                                 ] 4#                                                ] 4#                                                ] #                                               ] 8#                                              ] 10#                                             ] 10111#                                            ] 1#                                            ] 121314#                                           ] 14#                                           ] 14151#                                          ] 161718#                                         ] 18192#                                        ] 202122#                                       ] 22232#                                      ] 2#                                      ] 242526#                                     ] 26#                                     ] 26272#                                    ] 282930#                                   ] 30313#                                  ] 323334#                                 ] 34#                                 ] 34353#                                ] 3#                                ] 363738#                               ] 38394#                              ] 404142#                             ] 42434#                            ] 444546#                           ] 46#                           ] 46474#                          ] 4#                          ] 484950#                         ] 50515#                        ] 525354#                       ] 54555#                      ] 5#                      ] 565758#                     ] 58#                     ] 58596#                    ] 6#                    ] 606162#                   ] 62636#                  ] 646566#                 ] 66676#                ] 6#                ] 686970#               ] 70#               ] 70717#              ] 727374#             ] 74757#            ] 767778#           ] 78#           ] 78798#          ] 8#          ] 808182#         ] 82#         ] 82838#        ] 848586#       ] 86878#      ] 888990#     ] 90#     ] 90919#    ] 9#    ] 929394#   ] 94959#  ] 969798# ] 9899100#] 100%\n",
      "activation_collect_and_calc_th for sample: 3:  20%|▏| 4/20 [00:06<00:29,  1.82s/[                                                  ]                                                   ]                                                   ] #                                                 ] #                                                 ] 4#                                                ] 4#                                                ] #                                               ] 8#                                              ] 10#                                             ] 10111#                                            ] 1#                                            ] 121314#                                           ] 14#                                           ] 14151#                                          ] 161718#                                         ] 18192#                                        ] 202122#                                       ] 22232#                                      ] 2#                                      ] 242526#                                     ] 26#                                     ] 26272#                                    ] 282930#                                   ] 30313#                                  ] 323334#                                 ] 34#                                 ] 34353#                                ] 3#                                ] 363738#                               ] 38394#                              ] 404142#                             ] 42434#                            ] 444546#                           ] 46#                           ] 46474#                          ] 4#                          ] 484950#                         ] 50515#                        ] 525354#                       ] 54555#                      ] 5#                      ] 565758#                     ] 58#                     ] 58596#                    ] 6#                    ] 606162#                   ] 62636#                  ] 646566#                 ] 66676#                ] 6#                ] 686970#               ] 70#               ] 70717#              ] 727374#             ] 74757#            ] 767778#           ] 78#           ] 78798#          ] 8#          ] 808182#         ] 82#         ] 82838#        ] 848586#       ] 86878#      ] 888990#     ] 90#     ] 90919#    ] 9#    ] 929394#   ] 94959#  ] 969798# ] 9899100#] 100%\n",
      "activation_collect_and_calc_th for sample: 4:  25%|▎| 5/20 [00:08<00:28,  1.90s/[                                                  ]                                                   ]                                                   ] #                                                 ] #                                                 ] 4#                                                ] 4#                                                ] #                                               ] 8#                                              ] 10#                                             ] 10111#                                            ] 1#                                            ] 121314#                                           ] 14#                                           ] 14151#                                          ] 161718#                                         ] 18192#                                        ] 202122#                                       ] 22232#                                      ] 2#                                      ] 242526#                                     ] 26#                                     ] 26272#                                    ] 282930#                                   ] 30313#                                  ] 323334#                                 ] 34#                                 ] 34353#                                ] 3#                                ] 363738#                               ] 38394#                              ] 404142#                             ] 42434#                            ] 444546#                           ] 46#                           ] 46474#                          ] 4#                          ] 484950#                         ] 50515#                        ] 525354#                       ] 54555#                      ] 5#                      ] 565758#                     ] 58#                     ] 58596#                    ] 6#                    ] 606162#                   ] 62636#                  ] 646566#                 ] 66676#                ] 6#                ] 686970#               ] 70#               ] 70717#              ] 727374#             ] 74757#            ] 767778#           ] 78#           ] 78798#          ] 8#          ] 808182#         ] 82#         ] 82838#        ] 848586#       ] 86878#      ] 888990#     ] 90#     ] 90919#    ] 9#    ] 929394#   ] 94959#  ] 969798# ] 9899100#] 100%\n",
      "activation_collect_and_calc_th for sample: 5:  30%|▎| 6/20 [00:10<00:27,  1.95s/[                                                  ]                                                   ]                                                   ] #                                                 ] #                                                 ] 4#                                                ] 4#                                                ] #                                               ] 8#                                              ] 10#                                             ] 10111#                                            ] 1#                                            ] 121314#                                           ] 14#                                           ] 14151#                                          ] 161718#                                         ] 18192#                                        ] 202122#                                       ] 22232#                                      ] 2#                                      ] 242526#                                     ] 26#                                     ] 26272#                                    ] 282930#                                   ] 30313#                                  ] 323334#                                 ] 34#                                 ] 34353#                                ] 3#                                ] 363738#                               ] 38394#                              ] 404142#                             ] 42434#                            ] 444546#                           ] 46#                           ] 46474#                          ] 4#                          ] 484950#                         ] 50515#                        ] 525354#                       ] 54555#                      ] 5#                      ] 565758#                     ] 58#                     ] 58596#                    ] 6#                    ] 606162#                   ] 62636#                  ] 646566#                 ] 66676#                ] 6#                ] 686970#               ] 70#               ] 70717#              ] 727374#             ] 74757#            ] 767778#           ] 78#           ] 78798#          ] 8#          ] 808182#         ] 82#         ] 82838#        ] 848586#       ] 86878#      ] 888990#     ] 90#     ] 90919#    ] 9#    ] 929394#   ] 94959#  ] 969798# ] 9899100#] 100%\n",
      "activation_collect_and_calc_th for sample: 6:  35%|▎| 7/20 [00:12<00:25,  2.00s/[                                                  ]                                                   ]                                                   ] #                                                 ] #                                                 ] 4#                                                ] 4#                                                ] #                                               ] 8#                                              ] 10#                                             ] 10111#                                            ] 1#                                            ] 121314#                                           ] 14#                                           ] 14151#                                          ] 161718#                                         ] 18192#                                        ] 202122#                                       ] 22232#                                      ] 2#                                      ] 242526#                                     ] 26#                                     ] 26272#                                    ] 282930#                                   ] 30313#                                  ] 323334#                                 ] 34#                                 ] 34353#                                ] 3#                                ] 363738#                               ] 38394#                              ] 404142#                             ] 42434#                            ] 444546#                           ] 46#                           ] 46474#                          ] 4#                          ] 484950#                         ] 50515#                        ] 525354#                       ] 54555#                      ] 5#                      ] 565758#                     ] 58#                     ] 58596#                    ] 6#                    ] 606162#                   ] 62636#                  ] 646566#                 ] 66676#                ] 6#                ] 686970#               ] 70#               ] 70717#              ] 727374#             ] 74757#            ] 767778#           ] 78#           ] 78798#          ] 8#          ] 808182#         ] 82#         ] 82838#        ] 848586#       ] 86878#      ] 888990#     ] 90#     ] 90919#    ] 9#    ] 929394#   ] 94959#  ] 969798# ] 9899100#] 100%\n",
      "activation_collect_and_calc_th for sample: 7:  40%|▍| 8/20 [00:14<00:24,  2.00s/[                                                  ]                                                   ]                                                   ] #                                                 ] #                                                 ] 4#                                                ] 4#                                                ] #                                               ] 8#                                              ] 10#                                             ] 10111#                                            ] 1#                                            ] 121314#                                           ] 14#                                           ] 14151#                                          ] 161718#                                         ] 18192#                                        ] 202122#                                       ] 22232#                                      ] 2#                                      ] 242526#                                     ] 26#                                     ] 26272#                                    ] 282930#                                   ] 30313#                                  ] 323334#                                 ] 34#                                 ] 34353#                                ] 3#                                ] 363738#                               ] 38394#                              ] 404142#                             ] 42434#                            ] 444546#                           ] 46#                           ] 46474#                          ] 4#                          ] 484950#                         ] 50515#                        ] 525354#                       ] 54555#                      ] 5#                      ] 565758#                     ] 58#                     ] 58596#                    ] 6#                    ] 606162#                   ] 62636#                  ] 646566#                 ] 66676#                ] 6#                ] 686970#               ] 70#               ] 70717#              ] 727374#             ] 74757#            ] 767778#           ] 78#           ] 78798#          ] 8#          ] 808182#         ] 82#         ] 82838#        ] 848586#       ] 86878#      ] 888990#     ] 90#     ] 90919#    ] 9#    ] 929394#   ] 94959#  ] 969798# ] 9899100#] 100%\n",
      "activation_collect_and_calc_th for sample: 8:  45%|▍| 9/20 [00:16<00:21,  1.97s/[                                                  ]                                                   ]                                                   ] #                                                 ] #                                                 ] 4#                                                ] 4#                                                ] #                                               ] 8#                                              ] 10#                                             ] 10111#                                            ] 1#                                            ] 121314#                                           ] 14#                                           ] 14151#                                          ] 161718#                                         ] 18192#                                        ] 202122#                                       ] 22232#                                      ] 2#                                      ] 242526#                                     ] 26#                                     ] 26272#                                    ] 282930#                                   ] 30313#                                  ] 323334#                                 ] 34#                                 ] 34353#                                ] 3#                                ] 363738#                               ] 38394#                              ] 404142#                             ] 42434#                            ] 444546#                           ] 46#                           ] 46474#                          ] 4#                          ] 484950#                         ] 50515#                        ] 525354#                       ] 54555#                      ] 5#                      ] 565758#                     ] 58#                     ] 58596#                    ] 6#                    ] 606162#                   ] 62636#                  ] 646566#                 ] 66676#                ] 6#                ] 686970#               ] 70#               ] 70717#              ] 727374#             ] 74757#            ] 767778#           ] 78#           ] 78798#          ] 8#          ] 808182#         ] 82#         ] 82838#        ] 848586#       ] 86878#      ] 888990#     ] 90#     ] 90919#    ] 9#    ] 929394#   ] 94959#  ] 969798# ] 9899100#] 100%\n",
      "activation_collect_and_calc_th for sample: 9:  50%|▌| 10/20 [00:18<00:19,  1.98s[                                                  ]                                                   ]                                                   ] #                                                 ] #                                                 ] 4#                                                ] 4#                                                ] #                                               ] 8#                                              ] 10#                                             ] 10111#                                            ] 1#                                            ] 121314#                                           ] 14#                                           ] 14151#                                          ] 161718#                                         ] 18192#                                        ] 202122#                                       ] 22232#                                      ] 2#                                      ] 242526#                                     ] 26#                                     ] 26272#                                    ] 282930#                                   ] 30313#                                  ] 323334#                                 ] 34#                                 ] 34353#                                ] 3#                                ] 363738#                               ] 38394#                              ] 404142#                             ] 42434#                            ] 444546#                           ] 46#                           ] 46474#                          ] 4#                          ] 484950#                         ] 50515#                        ] 525354#                       ] 54555#                      ] 5#                      ] 565758#                     ] 58#                     ] 58596#                    ] 6#                    ] 606162#                   ] 62636#                  ] 646566#                 ] 66676#                ] 6#                ] 686970#               ] 70#               ] 70717#              ] 727374#             ] 74757#            ] 767778#           ] 78#           ] 78798#          ] 8#          ] 808182#         ] 82#         ] 82838#        ] 848586#       ] 86878#      ] 888990#     ] 90#     ] 90919#    ] 9#    ] 929394#   ] 94959#  ] 969798# ] 9899100#] 100%\n",
      "activation_collect_and_calc_th for sample: 10:  55%|▌| 11/20 [00:20<00:18,  2.00[                                                  ]                                                   ]                                                   ] #                                                 ] #                                                 ] 4#                                                ] 4#                                                ] #                                               ] 8#                                              ] 10#                                             ] 10111#                                            ] 1#                                            ] 121314#                                           ] 14#                                           ] 14151#                                          ] 161718#                                         ] 18192#                                        ] 202122#                                       ] 22232#                                      ] 2#                                      ] 242526#                                     ] 26#                                     ] 26272#                                    ] 282930#                                   ] 30313#                                  ] 323334#                                 ] 34#                                 ] 34353#                                ] 3#                                ] 363738#                               ] 38394#                              ] 404142#                             ] 42434#                            ] 444546#                           ] 46#                           ] 46474#                          ] 4#                          ] 484950#                         ] 50515#                        ] 525354#                       ] 54555#                      ] 5#                      ] 565758#                     ] 58#                     ] 58596#                    ] 6#                    ] 606162#                   ] 62636#                  ] 646566#                 ] 66676#                ] 6#                ] 686970#               ] 70#               ] 70717#              ] 727374#             ] 74757#            ] 767778#           ] 78#           ] 78798#          ] 8#          ] 808182#         ] 82#         ] 82838#        ] 848586#       ] 86878#      ] 888990#     ] 90#     ] 90919#    ] 9#    ] 929394#   ] 94959#  ] 969798# ] 9899100#] 100%\n",
      "activation_collect_and_calc_th for sample: 11:  60%|▌| 12/20 [00:22<00:15,  1.97[                                                  ]                                                   ]                                                   ] #                                                 ] #                                                 ] 4#                                                ] 4#                                                ] #                                               ] 8#                                              ] 10#                                             ] 10111#                                            ] 1#                                            ] 121314#                                           ] 14#                                           ] 14151#                                          ] 161718#                                         ] 18192#                                        ] 202122#                                       ] 22232#                                      ] 2#                                      ] 242526#                                     ] 26#                                     ] 26272#                                    ] 282930#                                   ] 30313#                                  ] 323334#                                 ] 34#                                 ] 34353#                                ] 3#                                ] 363738#                               ] 38394#                              ] 404142#                             ] 42434#                            ] 444546#                           ] 46#                           ] 46474#                          ] 4#                          ] 484950#                         ] 50515#                        ] 525354#                       ] 54555#                      ] 5#                      ] 565758#                     ] 58#                     ] 58596#                    ] 6#                    ] 606162#                   ] 62636#                  ] 646566#                 ] 66676#                ] 6#                ] 686970#               ] 70#               ] 70717#              ] 727374#             ] 74757#            ] 767778#           ] 78#           ] 78798#          ] 8#          ] 808182#         ] 82#         ] 82838#        ] 848586#       ] 86878#      ] 888990#     ] 90#     ] 90919#    ] 9#    ] 929394#   ] 94959#  ] 969798# ] 9899100#] 100%\n",
      "activation_collect_and_calc_th for sample: 12:  65%|▋| 13/20 [00:24<00:14,  2.00[                                                  ]                                                   ]                                                   ] #                                                 ] #                                                 ] 4#                                                ] 4#                                                ] #                                               ] 8#                                              ] 10#                                             ] 10111#                                            ] 1#                                            ] 121314#                                           ] 14#                                           ] 14151#                                          ] 161718#                                         ] 18192#                                        ] 202122#                                       ] 22232#                                      ] 2#                                      ] 242526#                                     ] 26#                                     ] 26272#                                    ] 282930#                                   ] 30313#                                  ] 323334#                                 ] 34#                                 ] 34353#                                ] 3#                                ] 363738#                               ] 38394#                              ] 404142#                             ] 42434#                            ] 444546#                           ] 46#                           ] 46474#                          ] 4#                          ] 484950#                         ] 50515#                        ] 525354#                       ] 54555#                      ] 5#                      ] 565758#                     ] 58#                     ] 58596#                    ] 6#                    ] 606162#                   ] 62636#                  ] 646566#                 ] 66676#                ] 6#                ] 686970#               ] 70#               ] 70717#              ] 727374#             ] 74757#            ] 767778#           ] 78#           ] 78798#          ] 8#          ] 808182#         ] 82#         ] 82838#        ] 848586#       ] 86878#      ] 888990#     ] 90#     ] 90919#    ] 9#    ] 929394#   ] 94959#  ] 969798# ] 9899100#] 100%\n",
      "activation_collect_and_calc_th for sample: 13:  70%|▋| 14/20 [00:26<00:12,  2.02[                                                  ]                                                   ]                                                   ] #                                                 ] #                                                 ] 4#                                                ] 4#                                                ] #                                               ] 8#                                              ] 10#                                             ] 10111#                                            ] 1#                                            ] 121314#                                           ] 14#                                           ] 14151#                                          ] 161718#                                         ] 18192#                                        ] 202122#                                       ] 22232#                                      ] 2#                                      ] 242526#                                     ] 26#                                     ] 26272#                                    ] 282930#                                   ] 30313#                                  ] 323334#                                 ] 34#                                 ] 34353#                                ] 3#                                ] 363738#                               ] 38394#                              ] 404142#                             ] 42434#                            ] 444546#                           ] 46#                           ] 46474#                          ] 4#                          ] 484950#                         ] 50515#                        ] 525354#                       ] 54555#                      ] 5#                      ] 565758#                     ] 58#                     ] 58596#                    ] 6#                    ] 606162#                   ] 62636#                  ] 646566#                 ] 66676#                ] 6#                ] 686970#               ] 70#               ] 70717#              ] 727374#             ] 74757#            ] 767778#           ] 78#           ] 78798#          ] 8#          ] 808182#         ] 82#         ] 82838#        ] 848586#       ] 86878#      ] 888990#     ] 90#     ] 90919#    ] 9#    ] 929394#   ] 94959#  ] 969798# ] 9899100#] 100%\n",
      "activation_collect_and_calc_th for sample: 14:  75%|▊| 15/20 [00:28<00:10,  2.06[                                                  ]                                                   ]                                                   ] #                                                 ] #                                                 ] 4#                                                ] 4#                                                ] #                                               ] 8#                                              ] 10#                                             ] 10111#                                            ] 1#                                            ] 121314#                                           ] 14#                                           ] 14151#                                          ] 161718#                                         ] 18192#                                        ] 202122#                                       ] 22232#                                      ] 2#                                      ] 242526#                                     ] 26#                                     ] 26272#                                    ] 282930#                                   ] 30313#                                  ] 323334#                                 ] 34#                                 ] 34353#                                ] 3#                                ] 363738#                               ] 38394#                              ] 404142#                             ] 42434#                            ] 444546#                           ] 46#                           ] 46474#                          ] 4#                          ] 484950#                         ] 50515#                        ] 525354#                       ] 54555#                      ] 5#                      ] 565758#                     ] 58#                     ] 58596#                    ] 6#                    ] 606162#                   ] 62636#                  ] 646566#                 ] 66676#                ] 6#                ] 686970#               ] 70#               ] 70717#              ] 727374#             ] 74757#            ] 767778#           ] 78#           ] 78798#          ] 8#          ] 808182#         ] 82#         ] 82838#        ] 848586#       ] 86878#      ] 888990#     ] 90#     ] 90919#    ] 9#    ] 929394#   ] 94959#  ] 969798# ] 9899100#] 100%\n",
      "activation_collect_and_calc_th for sample: 15:  80%|▊| 16/20 [00:31<00:08,  2.07[                                                  ]                                                   ]                                                   ] #                                                 ] #                                                 ] 4#                                                ] 4#                                                ] #                                               ] 8#                                              ] 10#                                             ] 10111#                                            ] 1#                                            ] 121314#                                           ] 14#                                           ] 14151#                                          ] 161718#                                         ] 18192#                                        ] 202122#                                       ] 22232#                                      ] 2#                                      ] 242526#                                     ] 26#                                     ] 26272#                                    ] 282930#                                   ] 30313#                                  ] 323334#                                 ] 34#                                 ] 34353#                                ] 3#                                ] 363738#                               ] 38394#                              ] 404142#                             ] 42434#                            ] 444546#                           ] 46#                           ] 46474#                          ] 4#                          ] 484950#                         ] 50515#                        ] 525354#                       ] 54555#                      ] 5#                      ] 565758#                     ] 58#                     ] 58596#                    ] 6#                    ] 606162#                   ] 62636#                  ] 646566#                 ] 66676#                ] 6#                ] 686970#               ] 70#               ] 70717#              ] 727374#             ] 74757#            ] 767778#           ] 78#           ] 78798#          ] 8#          ] 808182#         ] 82#         ] 82838#        ] 848586#       ] 86878#      ] 888990#     ] 90#     ] 90919#    ] 9#    ] 929394#   ] 94959#  ] 969798# ] 9899100#] 100%\n",
      "activation_collect_and_calc_th for sample: 16:  85%|▊| 17/20 [00:32<00:05,  1.98[                                                  ]                                                   ]                                                   ] #                                                 ] #                                                 ] 4#                                                ] 4#                                                ] #                                               ] 8#                                              ] 10#                                             ] 10111#                                            ] 1#                                            ] 121314#                                           ] 14#                                           ] 14151#                                          ] 161718#                                         ] 18192#                                        ] 202122#                                       ] 22232#                                      ] 2#                                      ] 242526#                                     ] 26#                                     ] 26272#                                    ] 282930#                                   ] 30313#                                  ] 323334#                                 ] 34#                                 ] 34353#                                ] 3#                                ] 363738#                               ] 38394#                              ] 404142#                             ] 42434#                            ] 444546#                           ] 46#                           ] 46474#                          ] 4#                          ] 484950#                         ] 50515#                        ] 525354#                       ] 54555#                      ] 5#                      ] 565758#                     ] 58#                     ] 58596#                    ] 6#                    ] 606162#                   ] 62636#                  ] 646566#                 ] 66676#                ] 6#                ] 686970#               ] 70#               ] 70717#              ] 727374#             ] 74757#            ] 767778#           ] 78#           ] 78798#          ] 8#          ] 808182#         ] 82#         ] 82838#        ] 848586#       ] 86878#      ] 888990#     ] 90#     ] 90919#    ] 9#    ] 929394#   ] 94959#  ] 969798# ] 9899100#] 100%\n",
      "activation_collect_and_calc_th for sample: 17:  90%|▉| 18/20 [00:34<00:04,  2.00[                                                  ]                                                   ]                                                   ] #                                                 ] #                                                 ] 4#                                                ] 4#                                                ] #                                               ] 8#                                              ] 10#                                             ] 10111#                                            ] 1#                                            ] 121314#                                           ] 14#                                           ] 14151#                                          ] 161718#                                         ] 18192#                                        ] 202122#                                       ] 22232#                                      ] 2#                                      ] 242526#                                     ] 26#                                     ] 26272#                                    ] 282930#                                   ] 30313#                                  ] 323334#                                 ] 34#                                 ] 34353#                                ] 3#                                ] 363738#                               ] 38394#                              ] 404142#                             ] 42434#                            ] 444546#                           ] 46#                           ] 46474#                          ] 4#                          ] 484950#                         ] 50515#                        ] 525354#                       ] 54555#                      ] 5#                      ] 565758#                     ] 58#                     ] 58596#                    ] 6#                    ] 606162#                   ] 62636#                  ] 646566#                 ] 66676#                ] 6#                ] 686970#               ] 70#               ] 70717#              ] 727374#             ] 74757#            ] 767778#           ] 78#           ] 78798#          ] 8#          ] 808182#         ] 82#         ] 82838#        ] 848586#       ] 86878#      ] 888990#     ] 90#     ] 90919#    ] 9#    ] 929394#   ] 94959#  ] 969798# ] 9899100#] 100%\n",
      "activation_collect_and_calc_th for sample: 18:  95%|▉| 19/20 [00:36<00:02,  2.02[                                                  ]                                                   ]                                                   ] #                                                 ] #                                                 ] 4#                                                ] 4#                                                ] #                                               ] 8#                                              ] 10#                                             ] 10111#                                            ] 1#                                            ] 121314#                                           ] 14#                                           ] 14151#                                          ] 161718#                                         ] 18192#                                        ] 202122#                                       ] 22232#                                      ] 2#                                      ] 242526#                                     ] 26#                                     ] 26272#                                    ] 282930#                                   ] 30313#                                  ] 323334#                                 ] 34#                                 ] 34353#                                ] 3#                                ] 363738#                               ] 38394#                              ] 404142#                             ] 42434#                            ] 444546#                           ] 46#                           ] 46474#                          ] 4#                          ] 484950#                         ] 50515#                        ] 525354#                       ] 54555#                      ] 5#                      ] 565758#                     ] 58#                     ] 58596#                    ] 6#                    ] 606162#                   ] 62636#                  ] 646566#                 ] 66676#                ] 6#                ] 686970#               ] 70#               ] 70717#              ] 727374#             ] 74757#            ] 767778#           ] 78#           ] 78798#          ] 8#          ] 808182#         ] 82#         ] 82838#        ] 848586#       ] 86878#      ] 888990#     ] 90#     ] 90919#    ] 9#    ] 929394#   ] 94959#  ] 969798# ] 9899100#] 100%\n",
      "activation_collect_and_calc_th for sample: 19: 100%|█| 20/20 [00:38<00:00,  1.99[                                                  ]                                                   ]                                                   ] #                                                 ] #                                                 ] 4#                                                ] 4#                                                ] #                                               ] 8#                                              ] 10#                                             ] 10111#                                            ] 1#                                            ] 121314#                                           ] 14#                                           ] 14151#                                          ] 161718#                                         ] 18192#                                        ] 202122#                                       ] 22232#                                      ] 2#                                      ] 242526#                                     ] 26#                                     ] 26272#                                    ] 282930#                                   ] 30313#                                  ] 323334#                                 ] 34#                                 ] 34353#                                ] 3#                                ] 363738#                               ] 38394#                              ] 404142#                             ] 42434#                            ] 444546#                           ] 46#                           ] 46474#                          ] 4#                          ] 484950#                         ] 50515#                        ] 525354#                       ] 54555#                      ] 5#                      ] 565758#                     ] 58#                     ] 58596#                    ] 6#                    ] 606162#                   ] 62636#                  ] 646566#                 ] 66676#                ] 6#                ] 686970#               ] 70#               ] 70717#              ] 727374#             ] 74757#            ] 767778#           ] 78#           ] 78798#          ] 8#          ] 808182#         ] 82#         ] 82838#        ] 848586#       ] 86878#      ] 888990#     ] 90#     ] 90919#    ] 9#    ] 929394#   ] 94959#  ] 969798# ] 9899100#] 100%\n",
      "activation_collect_and_calc_th for sample: 19: 100%|█| 20/20 [00:40<00:00,  2.05\n",
      "[2048] threshold: /model.23/cv3/conv/Conv_output_0_Conv: 100%|█| 143/143 [00:00<\n",
      "prepare data from 20\n",
      "tune op: /model.24/m.2/Conv_output_0_Conv: 100%|█| 143/143 [02:17<00:00,  1.04it\n",
      "auto tune end, run time:138.1779272556305\n",
      "convert to int8 model\n",
      "2024/11/24 09:30:11 - INFO : TPU-MLIR v1.12-20241105\n",
      "[Running]: tpuc-opt best.mlir --processor-assign=\"chip=cv181x mode=INT8 num_device=1 num_core=1 addr_mode=auto\" --import-calibration-table=\"file=best_cali_table asymmetric=False\" --processor-top-optimize --convert-top-to-tpu=\" asymmetric=False doWinograd=False ignore_f16_overflow=False q_group_size=0 matmul_perchannel=False\" --canonicalize --weight-fold -o best_cv181x_int8_sym_tpu.mlir\n",
      "ConvertUpsampleOp : top.Upsample succeed!\n",
      "ConvertUpsampleOp : top.Upsample succeed!\n",
      "[Success]: tpuc-opt best.mlir --processor-assign=\"chip=cv181x mode=INT8 num_device=1 num_core=1 addr_mode=auto\" --import-calibration-table=\"file=best_cali_table asymmetric=False\" --processor-top-optimize --convert-top-to-tpu=\" asymmetric=False doWinograd=False ignore_f16_overflow=False q_group_size=0 matmul_perchannel=False\" --canonicalize --weight-fold -o best_cv181x_int8_sym_tpu.mlir\n",
      "[CMD]: model_runner.py --input best_in_f32.npz --model best_cv181x_int8_sym_tpu.mlir --output best_cv181x_int8_sym_tpu_outputs.npz  \n",
      "[                                                  ]                                                   ]                                                   ] #                                                 ] #                                                 ] 4#                                                ] 4#                                                ] #                                               ] #                                               ] 8#                                              ] 8#                                              ] 10#                                             ] 10111#                                            ] 121314#                                           ] 14151#                                          ] 161718#                                         ] 18192#                                        ] 2#                                        ] 202122#                                       ] 22#                                       ] 22232#                                      ] 2#                                      ] 242526#                                     ] 26#                                     ] 26272#                                    ] 2#                                    ] 282930#                                   ] 30313#                                  ] 323334#                                 ] 34353#                                ] 363738#                               ] 38394#                              ] 4#                              ] 404142#                             ] 42#                             ] 42434#                            ] 4#                            ] 444546#                           ] 46#                           ] 46474#                          ] 4#                          ] 484950#                         ] 50515#                        ] 525354#                       ] 54555#                      ] 565758#                     ] 58596#                    ] 6#                    ] 606162#                   ] 62#                   ] 62636#                  ] 6#                  ] 646566#                 ] 66#                 ] 66676#                ] 6#                ] 686970#               ] 70717#              ] 727374#             ] 74757#            ] 767778#           ] 78798#          ] 8#          ] 808182#         ] 82#         ] 82838#        ] 8#        ] 848586#       ] 86#       ] 86878#      ] 8#      ] 888990#     ] 90919#    ] 929394#   ] 94959#  ] 969798# ] 9899100#] 100%\n",
      "[Running]: npz_tool.py compare best_cv181x_int8_sym_tpu_outputs.npz best_top_outputs.npz --tolerance 0.9,0.6 --except - -vv\n",
      "compare /model.24/m.2/Conv_output_0_Conv:  67%|█▎| 2/3 [00:00<00:00, 730.97it/s]\n",
      "\n",
      "[/model.24/m.0/Conv_output_0_Conv]      SIMILAR [PASSED]\n",
      "    (1, 18, 80, 80) float32 \n",
      "    cosine_similarity      = 0.999582\n",
      "    euclidean_similarity   = 0.970374\n",
      "    sqnr_similarity        = 30.540414\n",
      "[/model.24/m.1/Conv_output_0_Conv]      SIMILAR [PASSED]\n",
      "    (1, 18, 40, 40) float32 \n",
      "    cosine_similarity      = 0.999496\n",
      "    euclidean_similarity   = 0.968219\n",
      "    sqnr_similarity        = 29.996622\n",
      "[/model.24/m.2/Conv_output_0_Conv]      SIMILAR [PASSED]\n",
      "    (1, 18, 20, 20) float32 \n",
      "    cosine_similarity      = 0.999497\n",
      "    euclidean_similarity   = 0.967210\n",
      "    sqnr_similarity        = 29.837263\n",
      "3 compared\n",
      "3 passed\n",
      "  0 equal, 0 close, 3 similar\n",
      "0 failed\n",
      "  0 not equal, 0 not similar\n",
      "min_similiarity = (0.9994956851005554, 0.967209980293406, 29.837262630462646)\n",
      "Target    best_cv181x_int8_sym_tpu_outputs.npz\n",
      "Reference best_top_outputs.npz\n",
      "npz compare PASSED.\n",
      "compare /model.24/m.2/Conv_output_0_Conv: 100%|███| 3/3 [00:00<00:00, 31.88it/s]\n",
      "[Success]: npz_tool.py compare best_cv181x_int8_sym_tpu_outputs.npz best_top_outputs.npz --tolerance 0.9,0.6 --except - -vv\n",
      "[Running]: tpuc-opt best_cv181x_int8_sym_tpu.mlir --mlir-disable-threading --strip-io-quant=\"quant_input=True quant_output=False quant_input_list= quant_output_list=\" --processor-tpu-optimize --dev-parallel --weight-reorder  --subnet-divide=\"dynamic=False\" --op-reorder --future-update=\"rank=0 weight_list=\" --layer-group=\"opt=2 group_by_cores=auto compress_mode=none\" --core-parallel --address-assign -o best_cv181x_int8_sym_final.mlir \n",
      "==---------------------------==\n",
      "Run LayerGroupSearchPass : \n",
      "    Searching the optimal layer groups\n",
      "==---------------------------==\n",
      "\n",
      "=======================================================\n",
      "***** Dynamic Programming layer group with cluster ****\n",
      "=======================================================\n",
      "total num of base_group is 5\n",
      "clusters idx(size): 0(2), 2(2), 4(2), 6(2), 8(2), 10(2), 12(1), 13(1), 14(2), 16(2), 18(2), 20(2), 22(2), 24(2), 26(2), 28(2), 30(2), 32(1), 33(2), 35(1), 36(2), 38(2), 40(2), 42(2), 44(2), 46(2), 48(2), 50(2), 52(2), 54(2), 56(1), 57(2), 59(1), 60(2), 62(2), 64(1), 65(1), 66(2), 68(2), 70(1), 71(1), 72(2), 74(1), 75(1), 76(1), 77(1), 78(1), 79(1), 80(2), 82(1), 83(2), 85(2), 87(2), 89(2), 91(2), 93(1), 94(2), 96(2), 98(2), 100(2), 102(2), 104(2), 106(2), 108(1), 109(2), 111(2), 113(2), 115(2), 117(2), 119(2), 121(1), 122(1), 123(2), 125(1), 126(2), 128(2), 130(2), 132(1), 133(2), 135(1), 136(1), 137(1), 138(2), 140(2), \n",
      "process base group 0, layer_num=142, cluster_num=84\n",
      "Searching best group slices...\n",
      "[                                                  ]                                                   ] #                                                 ] 4#                                                ] #                                               ] 8#                                              ] 10#                                             ] 1#                                            ] 121314#                                           ] 141518#                                         ] 18192#                                        ] 20212#                                      ] 242526#                                     ] 2#                                    ] 282930#                                   ] 30313#                                  ] 32#                                 ] 34353#                                ] 36374#                              ] 404142#                             ] 424346#                           ] 46474#                          ] 48#                         ] 50515#                        ] 525354#                       ] 5#                      ] 565758#                     ] 58596#                    ] 60#                   ] 62636#                  ] 64656#                ] 686970#               ] 707174#             ] 74757#            ] 76#           ] 78798#          ] 808182#         ] 8#        ] 848586#       ] 868790#     ] 90919#    ] 92939#  ] 969798# ] 98#] 100%\n",
      "clusters idx(size): 0(1), \n",
      "process base group 1, layer_num=1, cluster_num=1\n",
      "clusters idx(size): 0(1), \n",
      "process base group 2, layer_num=1, cluster_num=1\n",
      "clusters idx(size): 0(1), \n",
      "process base group 3, layer_num=1, cluster_num=1\n",
      "clusters idx(size): 0(1), \n",
      "process base group 4, layer_num=1, cluster_num=1\n",
      "-------------------------------------------------------\n",
      "Consider redundant computation and gdma cost\n",
      "-------------------------------------------------------\n",
      "The final cost of the two group is 385599\n",
      "//// Group cost 385599, optimal cut idx 140\n",
      "The final cost of the two group is 359408\n",
      "//// Group cost 359408, optimal cut idx 139\n",
      "The final cost of the two group is 405645\n",
      "The final cost of the two group is 1152004\n",
      "//// Group cost 1152004, optimal cut idx 137\n",
      "The final cost of the two group is 918144\n",
      "//// Group cost 918144, optimal cut idx 136\n",
      "The final cost of the two group is 485676\n",
      "//// Group cost 485676, optimal cut idx 135\n",
      "The final cost of the two group is 518169\n",
      "The final cost of the two group is 602372\n",
      "The final cost of the two group is 2440913\n",
      "//// Group cost 2440913, optimal cut idx 132\n",
      "The final cost of the two group is 2211773\n",
      "//// Group cost 2211773, optimal cut idx 131\n",
      "The final cost of the two group is 1240300\n",
      "//// Group cost 1240300, optimal cut idx 130\n",
      "The final cost of the two group is 842255\n",
      "//// Group cost 842255, optimal cut idx 129\n",
      "The final cost of the two group is 798160\n",
      "//// Group cost 798160, optimal cut idx 128\n",
      "The final cost of the two group is 919184\n",
      "The final cost of the two group is 1240636\n",
      "The final cost of the two group is 2764251\n",
      "//// Group cost 2764251, optimal cut idx 125\n",
      "The final cost of the two group is 3808977\n",
      "//// Group cost 3808977, optimal cut idx 124\n",
      "The final cost of the two group is 3511315\n",
      "//// Group cost 3511315, optimal cut idx 121\n",
      "The final cost of the two group is 3559344\n",
      "The final cost of the two group is 3723854\n",
      "The final cost of the two group is 3481344\n",
      "//// Group cost 3481344, optimal cut idx 118\n",
      "The final cost of the two group is 2774526\n",
      "//// Group cost 2774526, optimal cut idx 117\n",
      "The final cost of the two group is 2753771\n",
      "//// Group cost 2753771, optimal cut idx 116\n",
      "The final cost of the two group is 2915828\n",
      "The final cost of the two group is 2747821\n",
      "//// Group cost 2747821, optimal cut idx 113\n",
      "The final cost of the two group is 2973510\n",
      "The final cost of the two group is 2986353\n",
      "The final cost of the two group is 6145260\n",
      "//// Group cost 6145260, optimal cut idx 110\n",
      "The final cost of the two group is 4752933\n",
      "//// Group cost 4752933, optimal cut idx 109\n",
      "The final cost of the two group is 4592381\n",
      "//// Group cost 4592381, optimal cut idx 108\n",
      "The final cost of the two group is 4517514\n",
      "//// Group cost 4517514, optimal cut idx 107\n",
      "The final cost of the two group is 4515935\n",
      "//// Group cost 4515935, optimal cut idx 106\n",
      "The final cost of the two group is 4485265\n",
      "//// Group cost 4485265, optimal cut idx 105\n",
      "The final cost of the two group is 4519275\n",
      "The final cost of the two group is 4456680\n",
      "//// Group cost 4456680, optimal cut idx 103\n",
      "The final cost of the two group is 4489697\n",
      "The final cost of the two group is 1999652\n",
      "//// Group cost 1999652, optimal cut idx 102\n",
      "The final cost of the two group is 1891573\n",
      "//// Group cost 1891573, optimal cut idx 101\n",
      "The final cost of the two group is 1914496\n",
      "The final cost of the two group is 1958582\n",
      "The final cost of the two group is 2030216\n",
      "The final cost of the two group is 3936494\n",
      "//// Group cost 3936494, optimal cut idx 97\n",
      "The final cost of the two group is 4386161\n",
      "//// Group cost 4386161, optimal cut idx 90\n",
      "The final cost of the two group is 4336210\n",
      "//// Group cost 4336210, optimal cut idx 89\n",
      "The final cost of the two group is 3355854\n",
      "//// Group cost 3355854, optimal cut idx 88\n",
      "The final cost of the two group is 3337911\n",
      "//// Group cost 3337911, optimal cut idx 87\n",
      "The final cost of the two group is 3648900\n",
      "The final cost of the two group is 1907956\n",
      "//// Group cost 1907956, optimal cut idx 82\n",
      "The final cost of the two group is 2675980\n",
      "//// Group cost 2675980, optimal cut idx 79\n",
      "The final cost of the two group is 2166144\n",
      "//// Group cost 2166144, optimal cut idx 78\n",
      "The final cost of the two group is 107844\n",
      "//// Group cost 107844, optimal cut idx 77\n",
      "The final cost of the two group is 215688\n",
      "//// Group cost 215688, optimal cut idx 76\n",
      "The final cost of the two group is 215688\n",
      "//// Group cost 215688, optimal cut idx 75\n",
      "The final cost of the two group is 617680\n",
      "//// Group cost 617680, optimal cut idx 74\n",
      "The final cost of the two group is 1427980\n",
      "//// Group cost 1427980, optimal cut idx 71\n",
      "The final cost of the two group is 918144\n",
      "//// Group cost 918144, optimal cut idx 70\n",
      "The final cost of the two group is 584615\n",
      "//// Group cost 584615, optimal cut idx 69\n",
      "The final cost of the two group is 613308\n",
      "The final cost of the two group is 619966\n",
      "The final cost of the two group is 678534\n",
      "The final cost of the two group is 2539852\n",
      "//// Group cost 2539852, optimal cut idx 65\n",
      "The final cost of the two group is 2256570\n",
      "//// Group cost 2256570, optimal cut idx 64\n",
      "The final cost of the two group is 1213499\n",
      "//// Group cost 1213499, optimal cut idx 63\n",
      "The final cost of the two group is 766372\n",
      "//// Group cost 766372, optimal cut idx 62\n",
      "The final cost of the two group is 780389\n",
      "The final cost of the two group is 951650\n",
      "The final cost of the two group is 5041100\n",
      "//// Group cost 5041100, optimal cut idx 59\n",
      "The final cost of the two group is 6147937\n",
      "//// Group cost 6147937, optimal cut idx 58\n",
      "The final cost of the two group is 3587955\n",
      "//// Group cost 3587955, optimal cut idx 55\n",
      "The final cost of the two group is 3805950\n",
      "The final cost of the two group is 3980091\n",
      "The final cost of the two group is 3556464\n",
      "//// Group cost 3556464, optimal cut idx 51\n",
      "The final cost of the two group is 2259018\n",
      "//// Group cost 2259018, optimal cut idx 50\n",
      "The final cost of the two group is 2238263\n",
      "//// Group cost 2238263, optimal cut idx 49\n",
      "The final cost of the two group is 2466168\n",
      "//// Group cost 2466168, optimal cut idx 48\n",
      "The final cost of the two group is 2303914\n",
      "//// Group cost 2303914, optimal cut idx 47\n",
      "The final cost of the two group is 2241107\n",
      "//// Group cost 2241107, optimal cut idx 46\n",
      "The final cost of the two group is 2302287\n",
      "//// Group cost 2302287, optimal cut idx 45\n",
      "The final cost of the two group is 2284724\n",
      "//// Group cost 2284724, optimal cut idx 44\n",
      "The final cost of the two group is 2546229\n",
      "//// Group cost 2546229, optimal cut idx 43\n",
      "The final cost of the two group is 2537711\n",
      "//// Group cost 2537711, optimal cut idx 42\n",
      "The final cost of the two group is 2284724\n",
      "//// Group cost 2284724, optimal cut idx 41\n",
      "The final cost of the two group is 2591453\n",
      "The final cost of the two group is 2807739\n",
      "//// Group cost 2807739, optimal cut idx 40\n",
      "The final cost of the two group is 2786984\n",
      "//// Group cost 2786984, optimal cut idx 39\n",
      "The final cost of the two group is 5056649\n",
      "//// Group cost 5056649, optimal cut idx 35\n",
      "The final cost of the two group is 5344499\n",
      "//// Group cost 5344499, optimal cut idx 34\n",
      "The final cost of the two group is 1451307\n",
      "//// Group cost 1451307, optimal cut idx 33\n",
      "The final cost of the two group is 1135043\n",
      "//// Group cost 1135043, optimal cut idx 32\n",
      "The final cost of the two group is 853505\n",
      "//// Group cost 853505, optimal cut idx 31\n",
      "The final cost of the two group is 959622\n",
      "The final cost of the two group is 969642\n",
      "The final cost of the two group is 1069597\n",
      "The final cost of the two group is 2929046\n",
      "//// Group cost 2929046, optimal cut idx 30\n",
      "The final cost of the two group is 2948766\n",
      "The final cost of the two group is 2861105\n",
      "//// Group cost 2861105, optimal cut idx 28\n",
      "The final cost of the two group is 2839538\n",
      "//// Group cost 2839538, optimal cut idx 27\n",
      "The final cost of the two group is 3032983\n",
      "The final cost of the two group is 4347266\n",
      "//// Group cost 4347266, optimal cut idx 26\n",
      "The final cost of the two group is 4326475\n",
      "//// Group cost 4326475, optimal cut idx 25\n",
      "The final cost of the two group is 4490932\n",
      "The final cost of the two group is 2723482\n",
      "//// Group cost 2723482, optimal cut idx 23\n",
      "The final cost of the two group is 2745983\n",
      "The final cost of the two group is 2676240\n",
      "//// Group cost 2676240, optimal cut idx 21\n",
      "The final cost of the two group is 2702185\n",
      "The final cost of the two group is 4944058\n",
      "//// Group cost 4944058, optimal cut idx 20\n",
      "The final cost of the two group is 4835403\n",
      "//// Group cost 4835403, optimal cut idx 19\n",
      "The final cost of the two group is 4913498\n",
      "The final cost of the two group is 4884626\n",
      "The final cost of the two group is 5103305\n",
      "The final cost of the two group is 6008159\n",
      "//// Group cost 6008159, optimal cut idx 18\n",
      "The final cost of the two group is 5837103\n",
      "//// Group cost 5837103, optimal cut idx 15\n",
      "The final cost of the two group is 5957837\n",
      "The final cost of the two group is 2001417\n",
      "//// Group cost 2001417, optimal cut idx 14\n",
      "The final cost of the two group is 1337498\n",
      "//// Group cost 1337498, optimal cut idx 13\n",
      "The final cost of the two group is 3115247\n",
      "//// Group cost 3115247, optimal cut idx 12\n",
      "The final cost of the two group is 3335564\n",
      "The final cost of the two group is 3336408\n",
      "The final cost of the two group is 3372655\n",
      "The final cost of the two group is 3344305\n",
      "The final cost of the two group is 3711981\n",
      "//// Group cost 3711981, optimal cut idx 9\n",
      "The final cost of the two group is 3600610\n",
      "//// Group cost 3600610, optimal cut idx 8\n",
      "The final cost of the two group is 3579704\n",
      "//// Group cost 3579704, optimal cut idx 7\n",
      "The final cost of the two group is 3647093\n",
      "The final cost of the two group is 5564602\n",
      "//// Group cost 5564602, optimal cut idx 6\n",
      "The final cost of the two group is 5346136\n",
      "//// Group cost 5346136, optimal cut idx 5\n",
      "The final cost of the two group is 5383096\n",
      "The final cost of the two group is 5399872\n",
      "The final cost of the two group is 5525701\n",
      "The final cost of the two group is 9634378\n",
      "//// Group cost 9634378, optimal cut idx 1\n",
      "The final cost of the two group is 9767615\n",
      "-------------------------------------------------------\n",
      "Merge cut idx to reduce gdma cost\n",
      "-------------------------------------------------------\n",
      "GroupMethod_process time:935225\n",
      "==---------------------------==\n",
      "Run GroupPostTransformPass : \n",
      "    Some transform after layer groups is determined\n",
      "==---------------------------==\n",
      "==---------------------------==\n",
      "Run TimeStepAssignmentPass : \n",
      "    Assign timestep task for each group.\n",
      "==---------------------------==\n",
      "==---------------------------==\n",
      "Run LocalMemoryAllocationPass : \n",
      "    Allocate local memory for all layer groups\n",
      "==---------------------------==\n",
      "==---------------------------==\n",
      "Run TimeStepCombinePass : \n",
      "    Combine time step for better parallel balance\n",
      "==---------------------------==\n",
      "==---------------------------==\n",
      "Run GroupDataMoveOverlapPass : \n",
      "    Overlap data move between two layer group\n",
      "==---------------------------==\n",
      "GmemAllocator use OpSizeOrderAssign\n",
      "[Success]: tpuc-opt best_cv181x_int8_sym_tpu.mlir --mlir-disable-threading --strip-io-quant=\"quant_input=True quant_output=False quant_input_list= quant_output_list=\" --processor-tpu-optimize --dev-parallel --weight-reorder  --subnet-divide=\"dynamic=False\" --op-reorder --future-update=\"rank=0 weight_list=\" --layer-group=\"opt=2 group_by_cores=auto compress_mode=none\" --core-parallel --address-assign -o best_cv181x_int8_sym_final.mlir \n",
      "[Running]: tpuc-opt best_cv181x_int8_sym_final.mlir --codegen=\"model_file=best_int8.cvimodel embed_debug_info=False model_version=latest bmodel_only=False\" -o /dev/null\n",
      "[Success]: tpuc-opt best_cv181x_int8_sym_final.mlir --codegen=\"model_file=best_int8.cvimodel embed_debug_info=False model_version=latest bmodel_only=False\" -o /dev/null\n",
      "[CMD]: model_runner.py --input best_in_f32.npz --model best_int8.cvimodel --output best_cv181x_int8_sym_model_outputs.npz  \n",
      "setenv:cv181x\n",
      "Start TPU Simulator for cv181x\n",
      "device[0] opened, 4294967296\n",
      "version: 1.4.0\n",
      "best Build at 2024-11-24 09:30:25 For platform cv181x\n",
      "Cmodel: bm_load_cmdbuf\n",
      "Max SharedMem size:5734400\n",
      "Cmodel: bm_run_cmdbuf\n",
      "device[0] closed\n",
      "[Running]: npz_tool.py compare best_cv181x_int8_sym_model_outputs.npz best_cv181x_int8_sym_tpu_outputs.npz --tolerance 0.99,0.90 --except - -vv\n",
      "compare /model.24/m.2/Conv_output_0_Conv_f32:  67%|▋| 2/3 [00:00<00:00, 584.45it\n",
      "\n",
      "[/model.24/m.0/Conv_output_0_Conv_f32]        EQUAL [PASSED]\n",
      "    (1, 18, 80, 80) float32 \n",
      "[/model.24/m.1/Conv_output_0_Conv_f32]        EQUAL [PASSED]\n",
      "    (1, 18, 40, 40) float32 \n",
      "[/model.24/m.2/Conv_output_0_Conv_f32]        EQUAL [PASSED]\n",
      "    (1, 18, 20, 20) float32 \n",
      "3 compared\n",
      "3 passed\n",
      "  3 equal, 0 close, 0 similar\n",
      "0 failed\n",
      "  0 not equal, 0 not similar\n",
      "min_similiarity = (1.0, 1.0, inf)\n",
      "Target    best_cv181x_int8_sym_model_outputs.npz\n",
      "Reference best_cv181x_int8_sym_tpu_outputs.npz\n",
      "npz compare PASSED.\n",
      "compare /model.24/m.2/Conv_output_0_Conv_f32: 100%|█| 3/3 [00:00<00:00, 34.87it/\n",
      "[Success]: npz_tool.py compare best_cv181x_int8_sym_model_outputs.npz best_cv181x_int8_sym_tpu_outputs.npz --tolerance 0.99,0.90 --except - -vv\n"
     ]
    }
   ],
   "source": [
    "!chmod +x convert_yolov5_to_cvimodel.sh && bash ./convert_yolov5_to_cvimodel.sh best \"/workspace/datasets/duck1k_yolo/images/val\" \"/workspace/datasets/duck1k_yolo/images/train/11770_116.jpg\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
